{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\r\n",
        "import azureml.core\r\n",
        "from azureml.core import Workspace\r\n",
        "\r\n",
        "# Load the workspace from the saved config file\r\n",
        "ws = Workspace.from_config()\r\n",
        "print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Ready to use Azure ML 1.49.0 to work with saurav_aml\n"
        }
      ],
      "execution_count": 79,
      "metadata": {
        "gather": {
          "logged": 1679512758168
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Dataset\r\n",
        "from azureml.data.datapath import DataPath\r\n",
        "\r\n",
        "default_ds = ws.get_default_datastore()\r\n",
        "\r\n",
        "if 'insurance dataset' not in ws.datasets:\r\n",
        "    Dataset.File.upload_directory(src_dir='data',\r\n",
        "                              target=DataPath(default_ds, 'insurance-data/')\r\n",
        "                              )\r\n",
        "\r\n",
        "    #Create a tabular dataset from the path on the datastore (this may take a short while)\r\n",
        "    tab_data_set = Dataset.Tabular.from_delimited_files(path=(default_ds, 'insurance-data/*.csv'))\r\n",
        "\r\n",
        "    # Register the tabular dataset\r\n",
        "    try:\r\n",
        "        tab_data_set = tab_data_set.register(workspace=ws, \r\n",
        "                                name='insurance dataset',\r\n",
        "                                description='insurance data',\r\n",
        "                                tags = {'format':'CSV'},\r\n",
        "                                create_new_version=True)\r\n",
        "        print('Dataset registered.')\r\n",
        "    except Exception as ex:\r\n",
        "        print(ex)\r\n",
        "else:\r\n",
        "    print('Dataset already registered.')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Dataset already registered.\n"
        }
      ],
      "execution_count": 80,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1679512759414
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\r\n",
        "# Create a folder for the pipeline step files\r\n",
        "experiment_folder = 'insurance_pipeline'\r\n",
        "os.makedirs(experiment_folder, exist_ok=True)\r\n",
        "\r\n",
        "print(experiment_folder)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "insurance_pipeline\n"
        }
      ],
      "execution_count": 81,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1679512760230
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $experiment_folder/prep_insurance.py\r\n",
        "# Import libraries\r\n",
        "import os\r\n",
        "import argparse\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from azureml.core import Run\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "\r\n",
        "# Get parameters\r\n",
        "parser = argparse.ArgumentParser()\r\n",
        "parser.add_argument(\"--input-data\", type=str, dest='raw_dataset_id', help='raw dataset')\r\n",
        "parser.add_argument('--prepped-data', type=str, dest='prepped_data', default='prepped_data', help='Folder for results')\r\n",
        "args = parser.parse_args()\r\n",
        "prep_folder = args.prepped_data\r\n",
        "\r\n",
        "# Get the experiment run context\r\n",
        "run = Run.get_context()\r\n",
        "\r\n",
        "# load the data (passed as an input dataset)\r\n",
        "print(\"Loading Data...\")\r\n",
        "df = run.input_datasets['raw_data'].to_pandas_dataframe()\r\n",
        "\r\n",
        "# Log raw row count\r\n",
        "row_count = (len(df))\r\n",
        "run.log('raw_rows', row_count)\r\n",
        "\r\n",
        "\r\n",
        "def Preprocessing(df):\r\n",
        "    \"\"\"Data Pre-processing\"\"\"\r\n",
        "    # if '?' in the datset which we have to remove by NaN Values\r\n",
        "    df = df.replace('?',np.NaN)\r\n",
        "\r\n",
        "    # missing value treatment using fillna\r\n",
        "\r\n",
        "    # we will replace the '?' by the most common collision type as we are unaware of the type.\r\n",
        "    df['collision_type'].fillna(df['collision_type'].mode()[0], inplace = True)\r\n",
        "\r\n",
        "    # It may be the case that there are no responses for property damage then we might take it as No property damage.\r\n",
        "    df['property_damage'].fillna('NO', inplace = True)\r\n",
        "\r\n",
        "    # again, if there are no responses fpr police report available then we might take it as No report available\r\n",
        "    df['police_report_available'].fillna('NO', inplace = True)\r\n",
        "\r\n",
        "    # let's extrat days, month and year from policy bind date\r\n",
        "    df['policy_bind_date'] = pd.to_datetime(df['policy_bind_date'], errors = 'coerce')\r\n",
        "\r\n",
        "    # let's encode the fraud report to numerical values\r\n",
        "    df['fraud_reported'] = df['fraud_reported'].replace(('Y','N'),(0,1))\r\n",
        "\r\n",
        "    # dropping unimportant columns\r\n",
        "    df = df.drop(columns = [\r\n",
        "        'policy_number', \r\n",
        "        'insured_zip', \r\n",
        "        'policy_bind_date', \r\n",
        "        'incident_date', \r\n",
        "        'incident_location', \r\n",
        "        '_c39', \r\n",
        "        'auto_year', \r\n",
        "        'incident_hour_of_the_day'])\r\n",
        "\r\n",
        "    numeric_data = df._get_numeric_data()\r\n",
        "    cat_data = df.select_dtypes(include=['object'])\r\n",
        "\r\n",
        "    # Normalize the numeric columns\r\n",
        "    scaler = MinMaxScaler()\r\n",
        "\r\n",
        "    num_data_clean = scaler.fit_transform(numeric_data)\r\n",
        "\r\n",
        "    clean_data = pd.concat([pd.get_dummies(cat_data), numeric_data], axis=1)\r\n",
        "\r\n",
        "    return clean_data\r\n",
        "\r\n",
        "dataPrep = Preprocessing(df)\r\n",
        "\r\n",
        "# remove nulls\r\n",
        "# insurance = insurance.dropna()\r\n",
        "\r\n",
        "# # Normalize the numeric columns\r\n",
        "# scaler = MinMaxScaler()\r\n",
        "# num_cols = ['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree']\r\n",
        "# diabetes[num_cols] = scaler.fit_transform(diabetes[num_cols])\r\n",
        "\r\n",
        "# # Log processed rows\r\n",
        "row_count = (len(df))\r\n",
        "run.log('processed_rows', row_count)\r\n",
        "\r\n",
        "# Save the prepped data\r\n",
        "print(\"Saving Data...\")\r\n",
        "os.makedirs(prep_folder, exist_ok=True)\r\n",
        "save_path_1 = os.path.join(prep_folder,'prep_data.csv')\r\n",
        "dataPrep.to_csv(save_path_1, index=False, header=True)\r\n",
        "\r\n",
        "# End the run\r\n",
        "run.complete()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting insurance_pipeline/prep_insurance.py\n"
        }
      ],
      "execution_count": 82,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $experiment_folder/train_insurance.py\r\n",
        "# Import libraries\r\n",
        "from azureml.core import Run, Model\r\n",
        "import argparse\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import joblib\r\n",
        "import os\r\n",
        "# from azureml.core import Run\r\n",
        "import argparse, joblib, os\r\n",
        "import argparse\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from sklearn.metrics import accuracy_score, recall_score, classification_report, cohen_kappa_score\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "# from sklearn.tree import RandomForestClassifier\r\n",
        "from sklearn.metrics import roc_auc_score\r\n",
        "from sklearn.metrics import roc_curve\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "# Get parameters\r\n",
        "parser = argparse.ArgumentParser()\r\n",
        "parser.add_argument(\"--training-data\", type=str, dest='training_data', help='training data')\r\n",
        "args = parser.parse_args()\r\n",
        "training_data = args.training_data\r\n",
        "\r\n",
        "# Get the experiment run context\r\n",
        "run = Run.get_context()\r\n",
        "\r\n",
        "# load the prepared data file in the training folder\r\n",
        "print(\"Loading Data...\")\r\n",
        "file_path = os.path.join(training_data,'prep_data.csv')\r\n",
        "data_prep = pd.read_csv(file_path)\r\n",
        "\r\n",
        "\r\n",
        "# Get parameters\r\n",
        "import argparse\r\n",
        "parser = argparse.ArgumentParser()\r\n",
        "parser.add_argument(\"--n_estimators\", type=int)\r\n",
        "parser.add_argument(\"--min_samples_leaf\", type=int)\r\n",
        "parser.add_argument(\"--datafolder\", type=str)\r\n",
        "\r\n",
        "args, unknown = parser.parse_known_args()\r\n",
        "\r\n",
        "ne = args.n_estimators\r\n",
        "msl = args.min_samples_leaf\r\n",
        "\r\n",
        "print(ne, msl)\r\n",
        "\r\n",
        "X = data_prep.iloc[:, 0:-1]\r\n",
        "\r\n",
        "y = data_prep.iloc[:, -1]\r\n",
        "\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=1234)\r\n",
        "\r\n",
        "# Baseline Random forest based Model\r\n",
        "# rfc = RandomForestClassifier(n_estimators=ne, min_samples_leaf=msl)\r\n",
        "rfc = RandomForestClassifier()\r\n",
        "\r\n",
        "rfcg = rfc.fit(X_train, y_train) # fit on training data\r\n",
        "Y_predict = rfcg.predict(X_test)\r\n",
        "\r\n",
        "# Get the probability score - Scored Probabilities\r\n",
        "Y_prob = rfcg.predict_proba(X_test)[:, 1]\r\n",
        "\r\n",
        "# Get Confusion matrix and the accuracy/score - Evaluate\r\n",
        "\r\n",
        "cm    = confusion_matrix(y_test, Y_predict)\r\n",
        "accuracy = accuracy_score(y_test, Y_predict)\r\n",
        "\r\n",
        "# Create the confusion matrix dictionary\r\n",
        "cm_dict = {\"schema_type\": \"confusion_matrix\",\r\n",
        "           \"schema_version\": \"v1\",\r\n",
        "           \"data\": {\"class_labels\": [\"N\", \"Y\"],\r\n",
        "                    \"matrix\": cm.tolist()}\r\n",
        "           }\r\n",
        "\r\n",
        "run.log(\"TotalObservations\", len(data_prep))\r\n",
        "run.log_confusion_matrix(\"ConfusionMatrix\", cm_dict)\r\n",
        "run.log(\"Accuracy\", accuracy)\r\n",
        "\r\n",
        "# # Save the model in the run outputs\r\n",
        "# os.makedirs('outputs', exist_ok=True)\r\n",
        "# joblib.dump(value=rfc, filename='outputs/insurance_model.pkl')\r\n",
        "\r\n",
        "# # Complete the run\r\n",
        "# run.complete()\r\n",
        "\r\n",
        "# Save the trained model in the outputs folder\r\n",
        "print(\"Saving model...\")\r\n",
        "os.makedirs('outputs', exist_ok=True)\r\n",
        "model_file = os.path.join('outputs', 'insurance_model.pkl')\r\n",
        "joblib.dump(value=rfc, filename=model_file)\r\n",
        "\r\n",
        "# Register the model\r\n",
        "print('Registering model...')\r\n",
        "Model.register(workspace=run.experiment.workspace,\r\n",
        "               model_path = model_file,\r\n",
        "               model_name = 'insurance_model',\r\n",
        "               tags={'Training context':'Pipeline'},\r\n",
        "               properties={'Accuracy': np.float(accuracy)})\r\n",
        "\r\n",
        "\r\n",
        "run.complete()\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting insurance_pipeline/train_insurance.py\n"
        }
      ],
      "execution_count": 83,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.compute import ComputeTarget, AmlCompute\r\n",
        "from azureml.core.compute_target import ComputeTargetException\r\n",
        "\r\n",
        "cluster_name = \"saurav-compute-cluster\"\r\n",
        "\r\n",
        "try:\r\n",
        "    # Check for existing compute target\r\n",
        "    pipeline_cluster = ComputeTarget(workspace=ws, name=cluster_name)\r\n",
        "    print('Found existing cluster, use it.')\r\n",
        "except ComputeTargetException:\r\n",
        "    # If it doesn't already exist, create it\r\n",
        "    try:\r\n",
        "        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2', max_nodes=2)\r\n",
        "        pipeline_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\r\n",
        "        pipeline_cluster.wait_for_completion(show_output=True)\r\n",
        "    except Exception as ex:\r\n",
        "        print(ex)\r\n",
        "    "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Found existing cluster, use it.\n"
        }
      ],
      "execution_count": 84,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1679512761327
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $experiment_folder/experiment_new_env.yml\r\n",
        "name: experiment_env\r\n",
        "dependencies:\r\n",
        "- python=3.8\r\n",
        "- scikit-learn\r\n",
        "- ipykernel\r\n",
        "- matplotlib\r\n",
        "- pandas\r\n",
        "- pip\r\n",
        "- pip:\r\n",
        "  - azureml-defaults\r\n",
        "  - pyarrow"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting insurance_pipeline/experiment_new_env.yml\n"
        }
      ],
      "execution_count": 85,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\r\n",
        "from azureml.core import Environment\r\n",
        "from azureml.core.runconfig import RunConfiguration\r\n",
        "\r\n",
        "# Create a Python environment for the experiment (from a .yml file)\r\n",
        "experiment_env = Environment.from_conda_specification(\"experiment_env\", experiment_folder + \"/experiment_new_env.yml\")\r\n",
        "\r\n",
        "# Register the environment \r\n",
        "experiment_env.register(workspace=ws)\r\n",
        "registered_env = Environment.get(ws, 'experiment_env')\r\n",
        "\r\n",
        "# Create a new runconfig object for the pipeline\r\n",
        "pipeline_run_config = RunConfiguration()\r\n",
        "\r\n",
        "# Use the compute you created above. \r\n",
        "pipeline_run_config.target = pipeline_cluster\r\n",
        "\r\n",
        "# Assign the environment to the run configuration\r\n",
        "pipeline_run_config.environment = registered_env\r\n",
        "\r\n",
        "print (\"Run configuration created.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Run configuration created.\n"
        }
      ],
      "execution_count": 86,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1679512762023
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.data import OutputFileDatasetConfig\r\n",
        "from azureml.pipeline.steps import PythonScriptStep\r\n",
        "\r\n",
        "# Get the training dataset\r\n",
        "insurance_ds = ws.datasets.get(\"insurance dataset\")\r\n",
        "\r\n",
        "# Create an OutputFileDatasetConfig (temporary Data Reference) for data passed from step 1 to step 2\r\n",
        "prepped_data = OutputFileDatasetConfig(\"prepped_data\")\r\n",
        "\r\n",
        "# Step 1, Run the data prep script\r\n",
        "prep_step = PythonScriptStep(name = \"Prepare Data\",\r\n",
        "                                source_directory = experiment_folder,\r\n",
        "                                script_name = \"prep_insurance.py\",\r\n",
        "                                arguments = ['--input-data', insurance_ds.as_named_input('raw_data'),\r\n",
        "                                             '--prepped-data', prepped_data],\r\n",
        "                                compute_target = pipeline_cluster,\r\n",
        "                                runconfig = pipeline_run_config,\r\n",
        "                                allow_reuse = True)\r\n",
        "\r\n",
        "# Step 2, run the training script\r\n",
        "train_step = PythonScriptStep(name = \"Train and Register Model\",\r\n",
        "                                source_directory = experiment_folder,\r\n",
        "                                script_name = \"train_insurance.py\",\r\n",
        "                                arguments = ['--training-data', prepped_data.as_input()],\r\n",
        "                                compute_target = pipeline_cluster,\r\n",
        "                                runconfig = pipeline_run_config,\r\n",
        "                                allow_reuse = True)\r\n",
        "\r\n",
        "print(\"Pipeline steps defined\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Pipeline steps defined\n"
        }
      ],
      "execution_count": 87,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1679512762300
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Experiment\r\n",
        "from azureml.pipeline.core import Pipeline\r\n",
        "from azureml.widgets import RunDetails\r\n",
        "\r\n",
        "# Construct the pipeline\r\n",
        "pipeline_steps = [prep_step, train_step]\r\n",
        "pipeline_new = Pipeline(workspace=ws, steps=pipeline_steps)\r\n",
        "print(\"Pipeline is built.\")\r\n",
        "\r\n",
        "# Create an experiment and run the pipeline\r\n",
        "experiment_new = Experiment(workspace=ws, name = 'saurav-insurance-pipeline')\r\n",
        "pipeline_run = experiment_new.submit(pipeline_new, regenerate_outputs=True)\r\n",
        "print(\"Pipeline submitted for execution.\")\r\n",
        "RunDetails(pipeline_run).show()\r\n",
        "pipeline_run.wait_for_completion(show_output=True)\r\n",
        "     "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Pipeline is built.\nCreated step Prepare Data [faed0c59][5745ece0-1ef8-4743-8004-3e4938e5cd67], (This step will run and generate new outputs)\nCreated step Train and Register Model [357ca16b][d93bedd5-e70c-4283-9949-cdc34d47ac0c], (This step will run and generate new outputs)\nSubmitted PipelineRun 0adc6e45-425c-4f08-9c9c-f7b4052b0d4a\nLink to Azure Machine Learning Portal: https://ml.azure.com/runs/0adc6e45-425c-4f08-9c9c-f7b4052b0d4a?wsid=/subscriptions/7c248226-48dc-4d36-baa0-0f0883669328/resourcegroups/saurv_01/workspaces/saurav_aml&tid=0563aea9-b886-4b62-9457-a85924a13bd1\nPipeline submitted for execution.\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "_PipelineWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', …",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "be821b3830c04be595ef38ece289399c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/aml.mini.widget.v1": "{\"status\": \"Running\", \"workbench_run_details_uri\": \"https://ml.azure.com/runs/0adc6e45-425c-4f08-9c9c-f7b4052b0d4a?wsid=/subscriptions/7c248226-48dc-4d36-baa0-0f0883669328/resourcegroups/saurv_01/workspaces/saurav_aml&tid=0563aea9-b886-4b62-9457-a85924a13bd1\", \"run_id\": \"0adc6e45-425c-4f08-9c9c-f7b4052b0d4a\", \"run_properties\": {\"run_id\": \"0adc6e45-425c-4f08-9c9c-f7b4052b0d4a\", \"created_utc\": \"2023-03-22T19:10:28.548981Z\", \"properties\": {\"azureml.runsource\": \"azureml.PipelineRun\", \"runSource\": \"SDK\", \"runType\": \"SDK\", \"azureml.parameters\": \"{}\", \"azureml.continue_on_step_failure\": \"False\", \"azureml.continue_on_failed_optional_input\": \"True\", \"azureml.pipelineComponent\": \"pipelinerun\"}, \"tags\": {}, \"end_time_utc\": null, \"status\": \"Running\", \"log_files\": {\"logs/azureml/executionlogs.txt\": \"https://sauravaml7390822349.blob.core.windows.net/azureml/ExperimentRun/dcid.0adc6e45-425c-4f08-9c9c-f7b4052b0d4a/logs/azureml/executionlogs.txt?sv=2019-07-07&sr=b&sig=hcLv0IhQ3xe%2Ffj2gBUyJe624hBDCcCNxcDadzcnHCvw%3D&skoid=d4a5ccdf-0157-4f3b-b8b0-16434308cc74&sktid=0563aea9-b886-4b62-9457-a85924a13bd1&skt=2023-03-22T17%3A16%3A30Z&ske=2023-03-24T01%3A26%3A30Z&sks=b&skv=2019-07-07&st=2023-03-22T19%3A00%3A50Z&se=2023-03-23T03%3A10%3A50Z&sp=r\", \"logs/azureml/stderrlogs.txt\": \"https://sauravaml7390822349.blob.core.windows.net/azureml/ExperimentRun/dcid.0adc6e45-425c-4f08-9c9c-f7b4052b0d4a/logs/azureml/stderrlogs.txt?sv=2019-07-07&sr=b&sig=OF2APFHwsn5IH10VLWnpoTjG7pOpxVTV2Iz%2FucADiI8%3D&skoid=d4a5ccdf-0157-4f3b-b8b0-16434308cc74&sktid=0563aea9-b886-4b62-9457-a85924a13bd1&skt=2023-03-22T17%3A16%3A30Z&ske=2023-03-24T01%3A26%3A30Z&sks=b&skv=2019-07-07&st=2023-03-22T19%3A00%3A50Z&se=2023-03-23T03%3A10%3A50Z&sp=r\", \"logs/azureml/stdoutlogs.txt\": \"https://sauravaml7390822349.blob.core.windows.net/azureml/ExperimentRun/dcid.0adc6e45-425c-4f08-9c9c-f7b4052b0d4a/logs/azureml/stdoutlogs.txt?sv=2019-07-07&sr=b&sig=C6FVipZ%2BqtvrA3%2BhNkaRtbH3upztPylZwhP7MGW8mys%3D&skoid=d4a5ccdf-0157-4f3b-b8b0-16434308cc74&sktid=0563aea9-b886-4b62-9457-a85924a13bd1&skt=2023-03-22T17%3A16%3A30Z&ske=2023-03-24T01%3A26%3A30Z&sks=b&skv=2019-07-07&st=2023-03-22T19%3A00%3A50Z&se=2023-03-23T03%3A10%3A50Z&sp=r\"}, \"log_groups\": [[\"logs/azureml/executionlogs.txt\", \"logs/azureml/stderrlogs.txt\", \"logs/azureml/stdoutlogs.txt\"]], \"run_duration\": \"0:01:28\", \"run_number\": \"1679512228\", \"run_queued_details\": {\"status\": \"Running\", \"details\": null}}, \"child_runs\": [{\"run_id\": \"1b6a37be-dd9f-4154-b10a-ecf8dec3d78f\", \"name\": \"Prepare Data\", \"status\": \"Finished\", \"start_time\": \"2023-03-22T19:10:40.696047Z\", \"created_time\": \"2023-03-22T19:10:31.347516Z\", \"end_time\": \"2023-03-22T19:11:05.464201Z\", \"duration\": \"0:00:34\", \"run_number\": 1679512231, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2023-03-22T19:10:31.347516Z\", \"is_reused\": \"\"}, {\"run_id\": \"6e731a38-e93d-4b38-a437-5320a5174a56\", \"name\": \"Train and Register Model\", \"status\": \"Running\", \"start_time\": \"2023-03-22T19:11:12.255435Z\", \"created_time\": \"2023-03-22T19:11:07.678031Z\", \"end_time\": \"2023-03-22T19:11:34.037304Z\", \"duration\": \"0:00:26\", \"run_number\": 1679512267, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2023-03-22T19:11:07.678031Z\", \"is_reused\": \"\"}], \"children_metrics\": {\"categories\": null, \"series\": null, \"metricName\": null}, \"run_metrics\": [], \"run_logs\": \"[2023-03-22 19:10:29Z] Submitting 1 runs, first five are: faed0c59:1b6a37be-dd9f-4154-b10a-ecf8dec3d78f\\n[2023-03-22 19:11:06Z] Completing processing run id 1b6a37be-dd9f-4154-b10a-ecf8dec3d78f.\\n[2023-03-22 19:11:06Z] Submitting 1 runs, first five are: 357ca16b:6e731a38-e93d-4b38-a437-5320a5174a56\\n\", \"graph\": {\"datasource_nodes\": {\"9cb96b16\": {\"node_id\": \"9cb96b16\", \"name\": \"insurance dataset\"}}, \"module_nodes\": {\"faed0c59\": {\"node_id\": \"faed0c59\", \"name\": \"Prepare Data\", \"status\": \"Finished\", \"_is_reused\": false, \"run_id\": \"1b6a37be-dd9f-4154-b10a-ecf8dec3d78f\"}, \"357ca16b\": {\"node_id\": \"357ca16b\", \"name\": \"Train and Register Model\", \"status\": \"Running\", \"_is_reused\": false, \"run_id\": \"6e731a38-e93d-4b38-a437-5320a5174a56\"}}, \"edges\": [{\"source_node_id\": \"9cb96b16\", \"source_node_name\": \"insurance dataset\", \"source_name\": \"data\", \"target_name\": \"raw_data\", \"dst_node_id\": \"faed0c59\", \"dst_node_name\": \"Prepare Data\"}, {\"source_node_id\": \"faed0c59\", \"source_node_name\": \"Prepare Data\", \"source_name\": \"prepped_data\", \"target_name\": \"input_799bd967\", \"dst_node_id\": \"357ca16b\", \"dst_node_name\": \"Train and Register Model\"}], \"child_runs\": [{\"run_id\": \"1b6a37be-dd9f-4154-b10a-ecf8dec3d78f\", \"name\": \"Prepare Data\", \"status\": \"Finished\", \"start_time\": \"2023-03-22T19:10:40.696047Z\", \"created_time\": \"2023-03-22T19:10:31.347516Z\", \"end_time\": \"2023-03-22T19:11:05.464201Z\", \"duration\": \"0:00:34\", \"run_number\": 1679512231, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2023-03-22T19:10:31.347516Z\", \"is_reused\": \"\"}, {\"run_id\": \"6e731a38-e93d-4b38-a437-5320a5174a56\", \"name\": \"Train and Register Model\", \"status\": \"Running\", \"start_time\": \"2023-03-22T19:11:12.255435Z\", \"created_time\": \"2023-03-22T19:11:07.678031Z\", \"end_time\": \"2023-03-22T19:11:34.037304Z\", \"duration\": \"0:00:26\", \"run_number\": 1679512267, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2023-03-22T19:11:07.678031Z\", \"is_reused\": \"\"}]}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.49.0\"}, \"loading\": false}"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "PipelineRunId: 0adc6e45-425c-4f08-9c9c-f7b4052b0d4a\nLink to Azure Machine Learning Portal: https://ml.azure.com/runs/0adc6e45-425c-4f08-9c9c-f7b4052b0d4a?wsid=/subscriptions/7c248226-48dc-4d36-baa0-0f0883669328/resourcegroups/saurv_01/workspaces/saurav_aml&tid=0563aea9-b886-4b62-9457-a85924a13bd1\nPipelineRun Status: NotStarted\nPipelineRun Status: Running\n\n\nStepRunId: 1b6a37be-dd9f-4154-b10a-ecf8dec3d78f\nLink to Azure Machine Learning Portal: https://ml.azure.com/runs/1b6a37be-dd9f-4154-b10a-ecf8dec3d78f?wsid=/subscriptions/7c248226-48dc-4d36-baa0-0f0883669328/resourcegroups/saurv_01/workspaces/saurav_aml&tid=0563aea9-b886-4b62-9457-a85924a13bd1\nStepRun( Prepare Data ) Status: NotStarted\nStepRun( Prepare Data ) Status: Running\n\nStepRun(Prepare Data) Execution Summary\n========================================\nStepRun( Prepare Data ) Status: Finished\n{'runId': '1b6a37be-dd9f-4154-b10a-ecf8dec3d78f', 'target': 'saurav-compute-cluster', 'status': 'Completed', 'startTimeUtc': '2023-03-22T19:10:40.696047Z', 'endTimeUtc': '2023-03-22T19:11:05.464201Z', 'services': {}, 'properties': {'ContentSnapshotId': 'c3ad4feb-0ee7-45df-8227-1f602f6c7989', 'StepType': 'PythonScriptStep', 'ComputeTargetType': 'AmlCompute', 'azureml.moduleid': '5745ece0-1ef8-4743-8004-3e4938e5cd67', 'azureml.moduleName': 'Prepare Data', 'azureml.runsource': 'azureml.StepRun', 'azureml.nodeid': 'faed0c59', 'azureml.pipelinerunid': '0adc6e45-425c-4f08-9c9c-f7b4052b0d4a', 'azureml.pipeline': '0adc6e45-425c-4f08-9c9c-f7b4052b0d4a', 'azureml.pipelineComponent': 'masterescloud', '_azureml.ComputeTargetType': 'amlctrain', 'ProcessInfoFile': 'azureml-logs/process_info.json', 'ProcessStatusFile': 'azureml-logs/process_status.json'}, 'inputDatasets': [{'dataset': {'id': '6da6cf53-ab30-4464-b645-a8b03cd8d2a3'}, 'consumptionDetails': {'type': 'RunInput', 'inputName': 'raw_data', 'mechanism': 'Direct'}}], 'outputDatasets': [{'identifier': {'savedId': 'caa4352f-7b4f-47c5-af88-fd5a91013cea'}, 'outputType': 'RunOutput', 'outputDetails': {'outputName': 'prepped_data'}, 'dataset': {\n  \"source\": [\n    \"('workspaceblobstore', 'dataset/1b6a37be-dd9f-4154-b10a-ecf8dec3d78f/prepped_data/')\"\n  ],\n  \"definition\": [\n    \"GetDatastoreFiles\"\n  ],\n  \"registration\": {\n    \"id\": \"caa4352f-7b4f-47c5-af88-fd5a91013cea\",\n    \"name\": null,\n    \"version\": null,\n    \"workspace\": \"Workspace.create(name='saurav_aml', subscription_id='7c248226-48dc-4d36-baa0-0f0883669328', resource_group='saurv_01')\"\n  }\n}}], 'runDefinition': {'script': 'prep_insurance.py', 'command': '', 'useAbsolutePath': False, 'arguments': ['--input-data', 'DatasetConsumptionConfig:raw_data', '--prepped-data', 'DatasetOutputConfig:prepped_data'], 'sourceDirectoryDataStore': None, 'framework': 'Python', 'communicator': 'None', 'target': 'saurav-compute-cluster', 'dataReferences': {}, 'data': {'raw_data': {'dataLocation': {'dataset': {'id': '6da6cf53-ab30-4464-b645-a8b03cd8d2a3', 'name': None, 'version': '1'}, 'dataPath': None, 'uri': None, 'type': None}, 'mechanism': 'Direct', 'environmentVariableName': 'raw_data', 'pathOnCompute': None, 'overwrite': False, 'options': None}}, 'outputData': {'prepped_data': {'outputLocation': {'dataset': None, 'dataPath': {'datastoreName': 'workspaceblobstore', 'relativePath': None}, 'uri': None, 'type': None}, 'mechanism': 'Mount', 'additionalOptions': {'pathOnCompute': None, 'registrationOptions': {'name': None, 'description': None, 'tags': None, 'properties': {'azureml.pipelineRunId': '0adc6e45-425c-4f08-9c9c-f7b4052b0d4a', 'azureml.pipelineRun.moduleNodeId': 'faed0c59', 'azureml.pipelineRun.outputPortName': 'prepped_data'}, 'datasetRegistrationOptions': {'additionalTransformation': None}}, 'uploadOptions': {'overwrite': False, 'sourceGlobs': {'globPatterns': None}}, 'mountOptions': None}, 'environmentVariableName': None}}, 'datacaches': [], 'jobName': None, 'maxRunDurationSeconds': None, 'nodeCount': 1, 'instanceTypes': [], 'priority': None, 'credentialPassthrough': False, 'identity': None, 'environment': {'name': 'experiment_env', 'version': '3', 'assetId': 'azureml://locations/eastus2/workspaces/9db2ef5f-31fb-4ace-8eb7-d0b7bd78bc35/environments/experiment_env/versions/3', 'autoRebuild': True, 'python': {'interpreterPath': 'python', 'userManagedDependencies': False, 'condaDependencies': {'name': 'experiment_env', 'dependencies': ['python=3.8', 'scikit-learn', 'ipykernel', 'matplotlib', 'pandas', 'pip', {'pip': ['azureml-defaults', 'pyarrow']}]}, 'baseCondaEnvironment': None}, 'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'}, 'docker': {'baseImage': 'mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:20230120.v1', 'platform': {'os': 'Linux', 'architecture': 'amd64'}, 'baseDockerfile': None, 'baseImageRegistry': {'address': None, 'username': None, 'password': None}, 'enabled': False, 'arguments': []}, 'spark': {'repositories': [], 'packages': [], 'precachePackages': True}, 'inferencingStackVersion': None}, 'history': {'outputCollection': True, 'directoriesToWatch': ['logs'], 'enableMLflowTracking': True, 'snapshotProject': True}, 'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment', 'spark.yarn.maxAppAttempts': '1'}}, 'parallelTask': {'maxRetriesPerWorker': 0, 'workerCountPerNode': 1, 'terminalExitCodes': None, 'configuration': {}}, 'amlCompute': {'name': None, 'vmSize': None, 'retainCluster': False, 'clusterMaxNodeCount': 1}, 'aiSuperComputer': {'instanceType': 'D2', 'imageVersion': 'pytorch-1.7.0', 'location': None, 'aiSuperComputerStorageData': None, 'interactive': False, 'scalePolicy': None, 'virtualClusterArmId': None, 'tensorboardLogDirectory': None, 'sshPublicKey': None, 'sshPublicKeys': None, 'enableAzmlInt': True, 'priority': 'Medium', 'slaTier': 'Standard', 'userAlias': None}, 'kubernetesCompute': {'instanceType': None}, 'tensorflow': {'workerCount': 1, 'parameterServerCount': 1}, 'mpi': {'processCountPerNode': 1}, 'pyTorch': {'communicationBackend': 'nccl', 'processCount': None}, 'hdi': {'yarnDeployMode': 'Cluster'}, 'containerInstance': {'region': None, 'cpuCores': 2.0, 'memoryGb': 3.5}, 'exposedPorts': None, 'docker': {'useDocker': False, 'sharedVolumes': True, 'shmSize': '2g', 'arguments': []}, 'cmk8sCompute': {'configuration': {}}, 'commandReturnCodeConfig': {'returnCode': 'Zero', 'successfulReturnCodes': []}, 'environmentVariables': {}, 'applicationEndpoints': {}, 'parameters': []}, 'logFiles': {'logs/azureml/dataprep/0/backgroundProcess.log': 'https://sauravaml7390822349.blob.core.windows.net/azureml/ExperimentRun/dcid.1b6a37be-dd9f-4154-b10a-ecf8dec3d78f/logs/azureml/dataprep/0/backgroundProcess.log?sv=2019-07-07&sr=b&sig=U4J9wp6OyaQZfLb85x3ymQJOZa5%2FuEAYRNgjOLRnKpI%3D&skoid=d4a5ccdf-0157-4f3b-b8b0-16434308cc74&sktid=0563aea9-b886-4b62-9457-a85924a13bd1&skt=2023-03-22T17%3A16%3A30Z&ske=2023-03-24T01%3A26%3A30Z&sks=b&skv=2019-07-07&st=2023-03-22T19%3A01%3A00Z&se=2023-03-23T03%3A11%3A00Z&sp=r', 'logs/azureml/dataprep/0/backgroundProcess_Telemetry.log': 'https://sauravaml7390822349.blob.core.windows.net/azureml/ExperimentRun/dcid.1b6a37be-dd9f-4154-b10a-ecf8dec3d78f/logs/azureml/dataprep/0/backgroundProcess_Telemetry.log?sv=2019-07-07&sr=b&sig=TnKDPqhOl27rxUk3ffcZ6fyE6MAOKQGoYHQeq4GhQx8%3D&skoid=d4a5ccdf-0157-4f3b-b8b0-16434308cc74&sktid=0563aea9-b886-4b62-9457-a85924a13bd1&skt=2023-03-22T17%3A16%3A30Z&ske=2023-03-24T01%3A26%3A30Z&sks=b&skv=2019-07-07&st=2023-03-22T19%3A01%3A00Z&se=2023-03-23T03%3A11%3A00Z&sp=r', 'logs/azureml/dataprep/0/rslex.log.2023-03-22-19': 'https://sauravaml7390822349.blob.core.windows.net/azureml/ExperimentRun/dcid.1b6a37be-dd9f-4154-b10a-ecf8dec3d78f/logs/azureml/dataprep/0/rslex.log.2023-03-22-19?sv=2019-07-07&sr=b&sig=%2BIKHR5HCSXbLTTjOpcy6UXoGVUf42zueGBD3kXxW4aQ%3D&skoid=d4a5ccdf-0157-4f3b-b8b0-16434308cc74&sktid=0563aea9-b886-4b62-9457-a85924a13bd1&skt=2023-03-22T17%3A16%3A30Z&ske=2023-03-24T01%3A26%3A30Z&sks=b&skv=2019-07-07&st=2023-03-22T19%3A01%3A00Z&se=2023-03-23T03%3A11%3A00Z&sp=r', 'logs/azureml/executionlogs.txt': 'https://sauravaml7390822349.blob.core.windows.net/azureml/ExperimentRun/dcid.1b6a37be-dd9f-4154-b10a-ecf8dec3d78f/logs/azureml/executionlogs.txt?sv=2019-07-07&sr=b&sig=Br%2FHiMs7Zun6IjOSryiScztgnyQ5GPEZDCMp4iXk6pw%3D&skoid=d4a5ccdf-0157-4f3b-b8b0-16434308cc74&sktid=0563aea9-b886-4b62-9457-a85924a13bd1&skt=2023-03-22T17%3A16%3A30Z&ske=2023-03-24T01%3A26%3A30Z&sks=b&skv=2019-07-07&st=2023-03-22T19%3A01%3A00Z&se=2023-03-23T03%3A11%3A00Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://sauravaml7390822349.blob.core.windows.net/azureml/ExperimentRun/dcid.1b6a37be-dd9f-4154-b10a-ecf8dec3d78f/logs/azureml/stderrlogs.txt?sv=2019-07-07&sr=b&sig=%2BUL3TAbe3x6vqsKI%2BxuPBBRtH0NMk6Xy6Jc2axL0u8I%3D&skoid=d4a5ccdf-0157-4f3b-b8b0-16434308cc74&sktid=0563aea9-b886-4b62-9457-a85924a13bd1&skt=2023-03-22T17%3A16%3A30Z&ske=2023-03-24T01%3A26%3A30Z&sks=b&skv=2019-07-07&st=2023-03-22T19%3A01%3A00Z&se=2023-03-23T03%3A11%3A00Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://sauravaml7390822349.blob.core.windows.net/azureml/ExperimentRun/dcid.1b6a37be-dd9f-4154-b10a-ecf8dec3d78f/logs/azureml/stdoutlogs.txt?sv=2019-07-07&sr=b&sig=MKY5py9JDn%2B2EaOcd2vfRus4I%2F%2FKkEocjCYCU1UR7mE%3D&skoid=d4a5ccdf-0157-4f3b-b8b0-16434308cc74&sktid=0563aea9-b886-4b62-9457-a85924a13bd1&skt=2023-03-22T17%3A16%3A30Z&ske=2023-03-24T01%3A26%3A30Z&sks=b&skv=2019-07-07&st=2023-03-22T19%3A01%3A00Z&se=2023-03-23T03%3A11%3A00Z&sp=r', 'user_logs/std_log.txt': 'https://sauravaml7390822349.blob.core.windows.net/azureml/ExperimentRun/dcid.1b6a37be-dd9f-4154-b10a-ecf8dec3d78f/user_logs/std_log.txt?sv=2019-07-07&sr=b&sig=HGWuVzOXvqBBeGCvBn4oEqpEVf4FqNNjT7Z%2FiK67S7A%3D&skoid=d4a5ccdf-0157-4f3b-b8b0-16434308cc74&sktid=0563aea9-b886-4b62-9457-a85924a13bd1&skt=2023-03-22T17%3A16%3A30Z&ske=2023-03-24T01%3A26%3A30Z&sks=b&skv=2019-07-07&st=2023-03-22T19%3A01%3A06Z&se=2023-03-23T03%3A11%3A06Z&sp=r', 'system_logs/cs_capability/cs-capability.log': 'https://sauravaml7390822349.blob.core.windows.net/azureml/ExperimentRun/dcid.1b6a37be-dd9f-4154-b10a-ecf8dec3d78f/system_logs/cs_capability/cs-capability.log?sv=2019-07-07&sr=b&sig=O1mi6OsMeGI0W8hm8dU3L4A17rOq7Tc6SnhPZz3gGUg%3D&skoid=d4a5ccdf-0157-4f3b-b8b0-16434308cc74&sktid=0563aea9-b886-4b62-9457-a85924a13bd1&skt=2023-03-22T17%3A16%3A30Z&ske=2023-03-24T01%3A26%3A30Z&sks=b&skv=2019-07-07&st=2023-03-22T19%3A01%3A06Z&se=2023-03-23T03%3A11%3A06Z&sp=r', 'system_logs/data_capability/data-capability.log': 'https://sauravaml7390822349.blob.core.windows.net/azureml/ExperimentRun/dcid.1b6a37be-dd9f-4154-b10a-ecf8dec3d78f/system_logs/data_capability/data-capability.log?sv=2019-07-07&sr=b&sig=TdvLqwbgB0%2FcrJV704jyDhKHYZqvES2A8XkooOwMz5s%3D&skoid=d4a5ccdf-0157-4f3b-b8b0-16434308cc74&sktid=0563aea9-b886-4b62-9457-a85924a13bd1&skt=2023-03-22T17%3A16%3A30Z&ske=2023-03-24T01%3A26%3A30Z&sks=b&skv=2019-07-07&st=2023-03-22T19%3A01%3A06Z&se=2023-03-23T03%3A11%3A06Z&sp=r', 'system_logs/data_capability/rslex.log.2023-03-22-19': 'https://sauravaml7390822349.blob.core.windows.net/azureml/ExperimentRun/dcid.1b6a37be-dd9f-4154-b10a-ecf8dec3d78f/system_logs/data_capability/rslex.log.2023-03-22-19?sv=2019-07-07&sr=b&sig=FSBp0LqNYbGzh9rxscFec%2BmzAe%2BDKpTI769Mr967q1c%3D&skoid=d4a5ccdf-0157-4f3b-b8b0-16434308cc74&sktid=0563aea9-b886-4b62-9457-a85924a13bd1&skt=2023-03-22T17%3A16%3A30Z&ske=2023-03-24T01%3A26%3A30Z&sks=b&skv=2019-07-07&st=2023-03-22T19%3A01%3A06Z&se=2023-03-23T03%3A11%3A06Z&sp=r', 'system_logs/hosttools_capability/hosttools-capability.log': 'https://sauravaml7390822349.blob.core.windows.net/azureml/ExperimentRun/dcid.1b6a37be-dd9f-4154-b10a-ecf8dec3d78f/system_logs/hosttools_capability/hosttools-capability.log?sv=2019-07-07&sr=b&sig=%2Fcjb1hgL%2Fm4DgupgVkRkPrSp5tOFJMUwKuULesADOL8%3D&skoid=d4a5ccdf-0157-4f3b-b8b0-16434308cc74&sktid=0563aea9-b886-4b62-9457-a85924a13bd1&skt=2023-03-22T17%3A16%3A30Z&ske=2023-03-24T01%3A26%3A30Z&sks=b&skv=2019-07-07&st=2023-03-22T19%3A01%3A06Z&se=2023-03-23T03%3A11%3A06Z&sp=r', 'system_logs/lifecycler/execution-wrapper.log': 'https://sauravaml7390822349.blob.core.windows.net/azureml/ExperimentRun/dcid.1b6a37be-dd9f-4154-b10a-ecf8dec3d78f/system_logs/lifecycler/execution-wrapper.log?sv=2019-07-07&sr=b&sig=IsuTUlCWemxjuL7SJjC%2F0WGh3k%2BPZoPjVm14tSoZYZs%3D&skoid=d4a5ccdf-0157-4f3b-b8b0-16434308cc74&sktid=0563aea9-b886-4b62-9457-a85924a13bd1&skt=2023-03-22T17%3A16%3A30Z&ske=2023-03-24T01%3A26%3A30Z&sks=b&skv=2019-07-07&st=2023-03-22T19%3A01%3A06Z&se=2023-03-23T03%3A11%3A06Z&sp=r', 'system_logs/lifecycler/lifecycler.log': 'https://sauravaml7390822349.blob.core.windows.net/azureml/ExperimentRun/dcid.1b6a37be-dd9f-4154-b10a-ecf8dec3d78f/system_logs/lifecycler/lifecycler.log?sv=2019-07-07&sr=b&sig=NDAVGGGTe1TKA6Mq5lV5Oc%2FFrPrTb1x8S%2B2ss7v2Cq0%3D&skoid=d4a5ccdf-0157-4f3b-b8b0-16434308cc74&sktid=0563aea9-b886-4b62-9457-a85924a13bd1&skt=2023-03-22T17%3A16%3A30Z&ske=2023-03-24T01%3A26%3A30Z&sks=b&skv=2019-07-07&st=2023-03-22T19%3A01%3A06Z&se=2023-03-23T03%3A11%3A06Z&sp=r', 'system_logs/metrics_capability/metrics-capability.log': 'https://sauravaml7390822349.blob.core.windows.net/azureml/ExperimentRun/dcid.1b6a37be-dd9f-4154-b10a-ecf8dec3d78f/system_logs/metrics_capability/metrics-capability.log?sv=2019-07-07&sr=b&sig=61Tx%2FIHkZWzKMvJUJ3AnhDMoayUszsH8lYMZb%2BhGz5Q%3D&skoid=d4a5ccdf-0157-4f3b-b8b0-16434308cc74&sktid=0563aea9-b886-4b62-9457-a85924a13bd1&skt=2023-03-22T17%3A16%3A30Z&ske=2023-03-24T01%3A26%3A30Z&sks=b&skv=2019-07-07&st=2023-03-22T19%3A01%3A06Z&se=2023-03-23T03%3A11%3A06Z&sp=r', 'system_logs/snapshot_capability/snapshot-capability.log': 'https://sauravaml7390822349.blob.core.windows.net/azureml/ExperimentRun/dcid.1b6a37be-dd9f-4154-b10a-ecf8dec3d78f/system_logs/snapshot_capability/snapshot-capability.log?sv=2019-07-07&sr=b&sig=OawWYaFQ6Zveqe7UT%2BOFpC9SYripaxEV9QMt%2FCpouJE%3D&skoid=d4a5ccdf-0157-4f3b-b8b0-16434308cc74&sktid=0563aea9-b886-4b62-9457-a85924a13bd1&skt=2023-03-22T17%3A16%3A30Z&ske=2023-03-24T01%3A26%3A30Z&sks=b&skv=2019-07-07&st=2023-03-22T19%3A01%3A06Z&se=2023-03-23T03%3A11%3A06Z&sp=r'}, 'submittedBy': 'saurav kumar'}\n\n\n\n\nStepRunId: 6e731a38-e93d-4b38-a437-5320a5174a56\nLink to Azure Machine Learning Portal: https://ml.azure.com/runs/6e731a38-e93d-4b38-a437-5320a5174a56?wsid=/subscriptions/7c248226-48dc-4d36-baa0-0f0883669328/resourcegroups/saurv_01/workspaces/saurav_aml&tid=0563aea9-b886-4b62-9457-a85924a13bd1\nStepRun( Train and Register Model ) Status: NotStarted\nStepRun( Train and Register Model ) Status: Running\n"
        }
      ],
      "execution_count": 88,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1679512553020
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for run in pipeline_run.get_children():\r\n",
        "    print(run.name, ':')\r\n",
        "    metrics = run.get_metrics()\r\n",
        "    for metric_name in metrics:\r\n",
        "        print('\\t',metric_name, \":\", metrics[metric_name])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1679512553937
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Model\r\n",
        "\r\n",
        "for model in Model.list(ws):\r\n",
        "    print(model.name, 'version:', model.version)\r\n",
        "    for tag_name in model.tags:\r\n",
        "        tag = model.tags[tag_name]\r\n",
        "        print ('\\t',tag_name, ':', tag)\r\n",
        "    for prop_name in model.properties:\r\n",
        "        prop = model.properties[prop_name]\r\n",
        "        print ('\\t',prop_name, ':', prop)\r\n",
        "    print('\\n')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1679512639855
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}