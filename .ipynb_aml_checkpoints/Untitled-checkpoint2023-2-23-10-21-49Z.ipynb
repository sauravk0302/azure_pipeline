{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\r\n",
        "import azureml.core\r\n",
        "from azureml.core import Workspace\r\n",
        "\r\n",
        "# Load the workspace from the saved config file\r\n",
        "ws = Workspace.from_config()\r\n",
        "print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Ready to use Azure ML 1.49.0 to work with saurav_aml\n"
        }
      ],
      "execution_count": 150,
      "metadata": {
        "gather": {
          "logged": 1679566507047
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Dataset\r\n",
        "from azureml.data.datapath import DataPath\r\n",
        "\r\n",
        "default_ds = ws.get_default_datastore()\r\n",
        "\r\n",
        "if 'insurance dataset' not in ws.datasets:\r\n",
        "    Dataset.File.upload_directory(src_dir='data',\r\n",
        "                              target=DataPath(default_ds, 'insurance-data/')\r\n",
        "                              )\r\n",
        "\r\n",
        "    #Create a tabular dataset from the path on the datastore (this may take a short while)\r\n",
        "    tab_data_set = Dataset.Tabular.from_delimited_files(path=(default_ds, 'insurance-data/*.csv'))\r\n",
        "\r\n",
        "    # Register the tabular dataset\r\n",
        "    try:\r\n",
        "        tab_data_set = tab_data_set.register(workspace=ws, \r\n",
        "                                name='insurance dataset',\r\n",
        "                                description='insurance data',\r\n",
        "                                tags = {'format':'CSV'},\r\n",
        "                                create_new_version=True)\r\n",
        "        print('Dataset registered.')\r\n",
        "    except Exception as ex:\r\n",
        "        print(ex)\r\n",
        "else:\r\n",
        "    print('Dataset already registered.')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Dataset already registered.\n"
        }
      ],
      "execution_count": 151,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1679566508141
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\r\n",
        "# Create a folder for the pipeline step files\r\n",
        "experiment_folder = 'insurance_pipeline'\r\n",
        "os.makedirs(experiment_folder, exist_ok=True)\r\n",
        "\r\n",
        "print(experiment_folder)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "insurance_pipeline\n"
        }
      ],
      "execution_count": 152,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1679566509786
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $experiment_folder/prep_insurance.py\r\n",
        "# Import libraries\r\n",
        "import os\r\n",
        "import argparse\r\n",
        "# import pandas as pd\r\n",
        "# import numpy as np\r\n",
        "from azureml.core import Run\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import scipy.stats as stats #It has all the probability distributions available along with many statistical functions.\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "# import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "# import seaborn as sns\r\n",
        "# import warnings\r\n",
        "# warnings.filterwarnings('ignore') # To supress warnings\r\n",
        "# sns.set(style=\"darkgrid\") # set the background for the graphs\r\n",
        "from scipy.stats import skew\r\n",
        "from statsmodels.stats.proportion import proportions_ztest # For proportion Z-test\r\n",
        "from statsmodels.formula.api import ols      # For n-way ANOVA\r\n",
        "from statsmodels.stats.anova import anova_lm # For n-way ANOVA\r\n",
        "from   scipy.stats import chi2_contingency   # For Chi-Sq \r\n",
        "\r\n",
        "\r\n",
        "# Get parameters\r\n",
        "parser = argparse.ArgumentParser()\r\n",
        "parser.add_argument(\"--input-data\", type=str, dest='raw_dataset_id', help='raw dataset')\r\n",
        "parser.add_argument('--prepped-data', type=str, dest='prepped_data', default='prepped_data', help='Folder for results')\r\n",
        "args = parser.parse_args()\r\n",
        "prep_folder = args.prepped_data\r\n",
        "\r\n",
        "# Get the experiment run context\r\n",
        "run = Run.get_context()\r\n",
        "\r\n",
        "# load the data (passed as an input dataset)\r\n",
        "print(\"Loading Data...\")\r\n",
        "df = run.input_datasets['raw_data'].to_pandas_dataframe()\r\n",
        "# df = df.replace('?',np.NaN)\r\n",
        "# Log raw row count\r\n",
        "row_count = (len(df))\r\n",
        "run.log('raw_rows', row_count)\r\n",
        "\r\n",
        "def iqr_outlier_cap(df,column):\r\n",
        "    q1 = df[column].quantile(0.25)\r\n",
        "    q2 = df[column].quantile(0.75)\r\n",
        "    #finding out the value of Inter Quartile Range\r\n",
        "    # IQR= np.subtract(q2,q1)\r\n",
        "    IQR= (q2.astype(np.float32) - q1.astype(np.float32)).astype(np.bool)\r\n",
        "    #defining max and min limits\r\n",
        "    max_limit = q2 + (1.5 * IQR)\r\n",
        "    min_limit = np.subtract(q1, (1.5 * IQR)) \r\n",
        "    #capping\r\n",
        "    df_new = pd.DataFrame(np.where(df[column] > max_limit, max_limit, \r\n",
        "             (np.where(df[column] < min_limit, min_limit, df[column]))), columns=[column])\r\n",
        "    return df_new\r\n",
        "\r\n",
        "def Preprocessing(df):\r\n",
        "    \"\"\"Data Pre-processing\"\"\"\r\n",
        "    # if '?' in the datset which we have to remove by NaN Values\r\n",
        "    df = df.replace('?',np.NaN)\r\n",
        "\r\n",
        "    df['collision_type'].fillna(df['collision_type'].mode()[0], inplace = True)\r\n",
        "    df['property_damage'].fillna('NO', inplace = True)\r\n",
        "    df['police_report_available'].fillna('NO', inplace = True)\r\n",
        "\r\n",
        "    # let's extrat days, month and year from policy bind date\r\n",
        "    # df['policy_bind_date'] = pd.to_datetime(df['policy_bind_date'], errors = 'coerce')\r\n",
        "\r\n",
        "    # dropping unimportant columns\r\n",
        "    # df = df.drop(columns = [\r\n",
        "    #     'umbrella_limit', \r\n",
        "    #     '_c39'])\r\n",
        "\r\n",
        "    df.drop(['_c39'], axis=1, inplace=True)    \r\n",
        "\r\n",
        "    numeric_data = df._get_numeric_data()\r\n",
        "    cat_data = df.select_dtypes(include=['object'])\r\n",
        "\r\n",
        "    lst=[]\r\n",
        "    for i in numeric_data.columns:\r\n",
        "        lst.append(iqr_outlier_cap(numeric_data,i))\r\n",
        "    numeric_data_cap=pd.concat(lst,axis=1)\r\n",
        "\r\n",
        "    for c in cat_data:\r\n",
        "        lbl = LabelEncoder()\r\n",
        "        lbl.fit(cat_data[c].values)\r\n",
        "        cat_data[c] = lbl.transform(cat_data[c].values)\r\n",
        "\r\n",
        "\r\n",
        "    # Normalize the numeric columns\r\n",
        "    # scaler = MinMaxScaler()\r\n",
        "\r\n",
        "    # num_data_clean = scaler.fit_transform(numeric_data_cap)\r\n",
        "\r\n",
        "    clean_data = pd.concat([numeric_data,cat_data],axis=1)\r\n",
        "\r\n",
        "    # clean_data = scaler.fit_transform(clean_data)\r\n",
        "    \r\n",
        "    \r\n",
        "    return clean_data\r\n",
        "\r\n",
        "dataPrep = Preprocessing(df)\r\n",
        "\r\n",
        "# # Log processed rows\r\n",
        "row_count = (len(df))\r\n",
        "run.log('processed_rows', row_count)\r\n",
        "\r\n",
        "# Save the prepped data\r\n",
        "print(\"Saving Data...\")\r\n",
        "os.makedirs(prep_folder, exist_ok=True)\r\n",
        "save_path_1 = os.path.join(prep_folder,'prep_data.csv')\r\n",
        "dataPrep.to_csv(save_path_1, index=False, header=True)\r\n",
        "\r\n",
        "# End the run\r\n",
        "run.complete()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting insurance_pipeline/prep_insurance.py\n"
        }
      ],
      "execution_count": 153,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $experiment_folder/train_insurance.py\r\n",
        "# Import libraries\r\n",
        "\r\n",
        "\r\n",
        "from azureml.core import Run, Model\r\n",
        "import argparse\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import joblib\r\n",
        "import os\r\n",
        "# from azureml.core import Run\r\n",
        "import argparse, joblib, os\r\n",
        "import argparse\r\n",
        "from imblearn.over_sampling import SMOTE\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from sklearn.metrics import accuracy_score, recall_score, classification_report, cohen_kappa_score\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "# from sklearn.tree import RandomForestClassifier\r\n",
        "from sklearn.metrics import roc_auc_score\r\n",
        "from sklearn.metrics import roc_curve\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "# Get parameters\r\n",
        "parser = argparse.ArgumentParser()\r\n",
        "parser.add_argument(\"--training-data\", type=str, dest='training_data', help='training data')\r\n",
        "args = parser.parse_args()\r\n",
        "training_data = args.training_data\r\n",
        "\r\n",
        "# Get the experiment run context\r\n",
        "run = Run.get_context()\r\n",
        "\r\n",
        "# load the prepared data file in the training folder\r\n",
        "print(\"Loading Data...\")\r\n",
        "file_path = os.path.join(training_data,'prep_data.csv')\r\n",
        "data_prep = pd.read_csv(file_path)\r\n",
        "\r\n",
        "\r\n",
        "# Get parameters\r\n",
        "import argparse\r\n",
        "parser = argparse.ArgumentParser()\r\n",
        "parser.add_argument(\"--n_estimators\", type=int)\r\n",
        "parser.add_argument(\"--min_samples_leaf\", type=int)\r\n",
        "parser.add_argument(\"--datafolder\", type=str)\r\n",
        "\r\n",
        "args, unknown = parser.parse_known_args()\r\n",
        "\r\n",
        "ne = args.n_estimators\r\n",
        "msl = args.min_samples_leaf\r\n",
        "\r\n",
        "print(ne, msl)\r\n",
        "\r\n",
        "# X = data_prep.iloc[:, 0:-1]\r\n",
        "\r\n",
        "# y = data_prep.iloc[:, -1]\r\n",
        "X = data_prep.drop(\"fraud_reported\",axis=1)\r\n",
        "y=data_prep[\"fraud_reported\"]\r\n",
        "\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=1234)\r\n",
        "#smote\r\n",
        "\r\n",
        "sm = SMOTE(random_state = 2)\r\n",
        "X_train, y_train= sm.fit_resample(X_train, y_train.ravel())\r\n",
        "\r\n",
        "# Baseline Random forest based Model\r\n",
        "# rfc = RandomForestClassifier(n_estimators=ne, min_samples_leaf=msl)\r\n",
        "rfc = RandomForestClassifier()\r\n",
        "\r\n",
        "rfcg = rfc.fit(X_train, y_train) # fit on training data\r\n",
        "Y_predict = rfcg.predict(X_test)\r\n",
        "\r\n",
        "# Get the probability score - Scored Probabilities\r\n",
        "Y_prob = rfcg.predict_proba(X_test)[:, 1]\r\n",
        "\r\n",
        "# Get Confusion matrix and the accuracy/score - Evaluate\r\n",
        "\r\n",
        "cm    = confusion_matrix(y_test, Y_predict)\r\n",
        "accuracy = accuracy_score(y_test, Y_predict)\r\n",
        "\r\n",
        "# Create the confusion matrix dictionary\r\n",
        "cm_dict = {\"schema_type\": \"confusion_matrix\",\r\n",
        "           \"schema_version\": \"v1\",\r\n",
        "           \"data\": {\"class_labels\": [\"N\", \"Y\"],\r\n",
        "                    \"matrix\": cm.tolist()}\r\n",
        "           }\r\n",
        "\r\n",
        "run.log(\"TotalObservations\", len(data_prep))\r\n",
        "run.log_confusion_matrix(\"ConfusionMatrix\", cm_dict)\r\n",
        "run.log(\"Accuracy\", accuracy)\r\n",
        "\r\n",
        "\r\n",
        "# Save the trained model in the outputs folder\r\n",
        "print(\"Saving model...\")\r\n",
        "os.makedirs('outputs', exist_ok=True)\r\n",
        "model_file = os.path.join('outputs', 'insurance_model.pkl')\r\n",
        "joblib.dump(value=rfc, filename=model_file)\r\n",
        "\r\n",
        "# Register the model\r\n",
        "print('Registering model...')\r\n",
        "Model.register(workspace=run.experiment.workspace,\r\n",
        "               model_path = model_file,\r\n",
        "               model_name = 'insurance_model',\r\n",
        "               tags={'Training context':'Pipeline'},\r\n",
        "               properties={'Accuracy': np.float(accuracy)})\r\n",
        "\r\n",
        "\r\n",
        "run.complete()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting insurance_pipeline/train_insurance.py\n"
        }
      ],
      "execution_count": 154,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.compute import ComputeTarget, AmlCompute\r\n",
        "from azureml.core.compute_target import ComputeTargetException\r\n",
        "\r\n",
        "cluster_name = \"saurav-compute-cluster\"\r\n",
        "\r\n",
        "try:\r\n",
        "    # Check for existing compute target\r\n",
        "    pipeline_cluster = ComputeTarget(workspace=ws, name=cluster_name)\r\n",
        "    print('Found existing cluster, use it.')\r\n",
        "except ComputeTargetException:\r\n",
        "    # If it doesn't already exist, create it\r\n",
        "    try:\r\n",
        "        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2', max_nodes=2)\r\n",
        "        pipeline_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\r\n",
        "        pipeline_cluster.wait_for_completion(show_output=True)\r\n",
        "    except Exception as ex:\r\n",
        "        print(ex)\r\n",
        "    "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Found existing cluster, use it.\n"
        }
      ],
      "execution_count": 155,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1679566511361
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $experiment_folder/experiment_env.yml\r\n",
        "name: experiment_env\r\n",
        "dependencies:\r\n",
        "- python=3.8\r\n",
        "- scikit-learn\r\n",
        "- ipykernel\r\n",
        "- matplotlib\r\n",
        "- pandas\r\n",
        "- numpy\r\n",
        "- statsmodels\r\n",
        "- scipy\r\n",
        "- pip\r\n",
        "- pip:\r\n",
        "  - azureml-defaults\r\n",
        "  - pyarrow\r\n",
        "  - imblearn\r\n",
        "  "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting insurance_pipeline/experiment_env.yml\n"
        }
      ],
      "execution_count": 156,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\r\n",
        "from azureml.core import Environment\r\n",
        "from azureml.core.runconfig import RunConfiguration\r\n",
        "\r\n",
        "# Create a Python environment for the experiment (from a .yml file)\r\n",
        "experiment_env = Environment.from_conda_specification(\"experiment_env\", experiment_folder + \"/experiment_env.yml\")\r\n",
        "\r\n",
        "# Register the environment \r\n",
        "experiment_env.register(workspace=ws)\r\n",
        "registered_env = Environment.get(ws, 'experiment_env')\r\n",
        "\r\n",
        "# Create a new runconfig object for the pipeline\r\n",
        "pipeline_run_config = RunConfiguration()\r\n",
        "\r\n",
        "# Use the compute you created above. \r\n",
        "pipeline_run_config.target = pipeline_cluster\r\n",
        "\r\n",
        "# Assign the environment to the run configuration\r\n",
        "pipeline_run_config.environment = registered_env\r\n",
        "\r\n",
        "print (\"Run configuration created.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Run configuration created.\n"
        }
      ],
      "execution_count": 157,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1679566512732
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.data import OutputFileDatasetConfig\r\n",
        "from azureml.pipeline.steps import PythonScriptStep\r\n",
        "\r\n",
        "# Get the training dataset\r\n",
        "insurance_ds = ws.datasets.get(\"insurance dataset\")\r\n",
        "\r\n",
        "# Create an OutputFileDatasetConfig (temporary Data Reference) for data passed from step 1 to step 2\r\n",
        "prepped_data = OutputFileDatasetConfig(\"prepped_data\")\r\n",
        "\r\n",
        "# Step 1, Run the data prep script\r\n",
        "prep_step = PythonScriptStep(name = \"Prepare Data\",\r\n",
        "                                source_directory = experiment_folder,\r\n",
        "                                script_name = \"prep_insurance.py\",\r\n",
        "                                arguments = ['--input-data', insurance_ds.as_named_input('raw_data'),\r\n",
        "                                             '--prepped-data', prepped_data],\r\n",
        "                                compute_target = pipeline_cluster,\r\n",
        "                                runconfig = pipeline_run_config,\r\n",
        "                                allow_reuse = True)\r\n",
        "\r\n",
        "# Step 2, run the training script\r\n",
        "train_step = PythonScriptStep(name = \"Train and Register Model\",\r\n",
        "                                source_directory = experiment_folder,\r\n",
        "                                script_name = \"train_insurance.py\",\r\n",
        "                                arguments = ['--training-data', prepped_data.as_input()],\r\n",
        "                                compute_target = pipeline_cluster,\r\n",
        "                                runconfig = pipeline_run_config,\r\n",
        "                                allow_reuse = True)\r\n",
        "\r\n",
        "print(\"Pipeline steps defined\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Pipeline steps defined\n"
        }
      ],
      "execution_count": 158,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1679566513889
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Experiment\r\n",
        "from azureml.pipeline.core import Pipeline\r\n",
        "from azureml.widgets import RunDetails\r\n",
        "\r\n",
        "# Construct the pipeline\r\n",
        "pipeline_steps = [prep_step, train_step]\r\n",
        "pipeline_new = Pipeline(workspace=ws, steps=pipeline_steps)\r\n",
        "print(\"Pipeline is built.\")\r\n",
        "\r\n",
        "# Create an experiment and run the pipeline\r\n",
        "experiment_new = Experiment(workspace=ws, name = 'saurav-insurance-pipeline')\r\n",
        "pipeline_run = experiment_new.submit(pipeline_new, regenerate_outputs=True)\r\n",
        "print(\"Pipeline submitted for execution.\")\r\n",
        "RunDetails(pipeline_run).show()\r\n",
        "pipeline_run.wait_for_completion(show_output=True)\r\n",
        "     "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Pipeline is built.\nCreated step Prepare Data [8e3aa793][45940dff-b098-480f-ae27-d12d8f10d9e1], (This step will run and generate new outputs)\nCreated step Train and Register Model [a99673a2][daee9143-c396-4d2f-9e9c-7a014e21e1db], (This step will run and generate new outputs)\nSubmitted PipelineRun 4d0b4e07-0eba-4770-bcf0-6a8b7ef331f2\nLink to Azure Machine Learning Portal: https://ml.azure.com/runs/4d0b4e07-0eba-4770-bcf0-6a8b7ef331f2?wsid=/subscriptions/7c248226-48dc-4d36-baa0-0f0883669328/resourcegroups/saurv_01/workspaces/saurav_aml&tid=0563aea9-b886-4b62-9457-a85924a13bd1\nPipeline submitted for execution.\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "_PipelineWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', â€¦",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b8fc259ae0f64925b392cab6562966ae"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/aml.mini.widget.v1": "{\"status\": \"Failed\", \"workbench_run_details_uri\": \"https://ml.azure.com/runs/4d0b4e07-0eba-4770-bcf0-6a8b7ef331f2?wsid=/subscriptions/7c248226-48dc-4d36-baa0-0f0883669328/resourcegroups/saurv_01/workspaces/saurav_aml&tid=0563aea9-b886-4b62-9457-a85924a13bd1\", \"run_id\": \"4d0b4e07-0eba-4770-bcf0-6a8b7ef331f2\", \"run_properties\": {\"run_id\": \"4d0b4e07-0eba-4770-bcf0-6a8b7ef331f2\", \"created_utc\": \"2023-03-23T10:06:16.925832Z\", \"properties\": {\"azureml.runsource\": \"azureml.PipelineRun\", \"runSource\": \"SDK\", \"runType\": \"SDK\", \"azureml.parameters\": \"{}\", \"azureml.continue_on_step_failure\": \"False\", \"azureml.continue_on_failed_optional_input\": \"True\", \"azureml.pipelineComponent\": \"pipelinerun\", \"azureml.pipelines.stages\": \"{\\\"Initialization\\\":null,\\\"Execution\\\":{\\\"StartTime\\\":\\\"2023-03-23T10:06:18.0946944+00:00\\\",\\\"EndTime\\\":\\\"2023-03-23T10:06:58.995849+00:00\\\",\\\"Status\\\":\\\"Failed\\\"}}\"}, \"tags\": {}, \"end_time_utc\": \"2023-03-23T10:06:59.103205Z\", \"status\": \"Failed\", \"log_files\": {\"logs/azureml/executionlogs.txt\": \"https://sauravaml7390822349.blob.core.windows.net/azureml/ExperimentRun/dcid.4d0b4e07-0eba-4770-bcf0-6a8b7ef331f2/logs/azureml/executionlogs.txt?sv=2019-07-07&sr=b&sig=cwvGCCiCVsunWslPGFrOBvuVoiwvWpSrnNqMQ%2FfXDdk%3D&skoid=d4a5ccdf-0157-4f3b-b8b0-16434308cc74&sktid=0563aea9-b886-4b62-9457-a85924a13bd1&skt=2023-03-23T07%3A56%3A28Z&ske=2023-03-24T16%3A06%3A28Z&sks=b&skv=2019-07-07&st=2023-03-23T10%3A01%3A56Z&se=2023-03-23T18%3A11%3A56Z&sp=r\", \"logs/azureml/stderrlogs.txt\": \"https://sauravaml7390822349.blob.core.windows.net/azureml/ExperimentRun/dcid.4d0b4e07-0eba-4770-bcf0-6a8b7ef331f2/logs/azureml/stderrlogs.txt?sv=2019-07-07&sr=b&sig=OzQk2W8aJn3CrbMKPdMzNioaJkqU85XH31SfUWojlJM%3D&skoid=d4a5ccdf-0157-4f3b-b8b0-16434308cc74&sktid=0563aea9-b886-4b62-9457-a85924a13bd1&skt=2023-03-23T07%3A56%3A28Z&ske=2023-03-24T16%3A06%3A28Z&sks=b&skv=2019-07-07&st=2023-03-23T10%3A01%3A56Z&se=2023-03-23T18%3A11%3A56Z&sp=r\", \"logs/azureml/stdoutlogs.txt\": \"https://sauravaml7390822349.blob.core.windows.net/azureml/ExperimentRun/dcid.4d0b4e07-0eba-4770-bcf0-6a8b7ef331f2/logs/azureml/stdoutlogs.txt?sv=2019-07-07&sr=b&sig=PVEhrwvPkwSusleZdx12sHQ92g2%2FQzvn7KwODb48KGY%3D&skoid=d4a5ccdf-0157-4f3b-b8b0-16434308cc74&sktid=0563aea9-b886-4b62-9457-a85924a13bd1&skt=2023-03-23T07%3A56%3A28Z&ske=2023-03-24T16%3A06%3A28Z&sks=b&skv=2019-07-07&st=2023-03-23T10%3A01%3A56Z&se=2023-03-23T18%3A11%3A56Z&sp=r\"}, \"log_groups\": [[\"logs/azureml/executionlogs.txt\", \"logs/azureml/stderrlogs.txt\", \"logs/azureml/stdoutlogs.txt\"]], \"run_duration\": \"0:00:42\", \"run_number\": \"1679565976\", \"run_queued_details\": {\"status\": \"Failed\", \"details\": null}}, \"child_runs\": [{\"run_id\": \"e367da67-c468-406c-b731-c6cd56843004\", \"name\": \"Prepare Data\", \"status\": \"Failed\", \"start_time\": \"2023-03-23T10:06:31.805515Z\", \"created_time\": \"2023-03-23T10:06:20.372822Z\", \"end_time\": \"2023-03-23T10:06:58.113305Z\", \"duration\": \"0:00:37\", \"run_number\": 1679565980, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2023-03-23T10:06:20.372822Z\", \"is_reused\": \"\"}, {\"run_id\": \"\", \"name\": \"Train and Register Model\", \"status\": \"NotStarted\", \"start_time\": \"\", \"created_time\": \"\", \"end_time\": \"\", \"duration\": \"\"}], \"children_metrics\": {\"categories\": null, \"series\": null, \"metricName\": null}, \"run_metrics\": [], \"run_logs\": \"[2023-03-23 10:06:18Z] Submitting 1 runs, first five are: 8e3aa793:e367da67-c468-406c-b731-c6cd56843004\\n[2023-03-23 10:06:58Z] Execution of experiment failed, update experiment status and cancel running nodes.\\n\\nError occurred: Pipeline has failed child jobs. For more details and logs, please go to the job detail page and check the child jobs.\\n\", \"graph\": {\"datasource_nodes\": {\"933677c2\": {\"node_id\": \"933677c2\", \"name\": \"insurance dataset\"}}, \"module_nodes\": {\"8e3aa793\": {\"node_id\": \"8e3aa793\", \"name\": \"Prepare Data\", \"status\": \"Failed\", \"_is_reused\": false, \"run_id\": \"e367da67-c468-406c-b731-c6cd56843004\"}, \"a99673a2\": {\"node_id\": \"a99673a2\", \"name\": \"Train and Register Model\", \"status\": \"NotStarted\"}}, \"edges\": [{\"source_node_id\": \"933677c2\", \"source_node_name\": \"insurance dataset\", \"source_name\": \"data\", \"target_name\": \"raw_data\", \"dst_node_id\": \"8e3aa793\", \"dst_node_name\": \"Prepare Data\"}, {\"source_node_id\": \"8e3aa793\", \"source_node_name\": \"Prepare Data\", \"source_name\": \"prepped_data\", \"target_name\": \"input_0959dc2a\", \"dst_node_id\": \"a99673a2\", \"dst_node_name\": \"Train and Register Model\"}], \"child_runs\": [{\"run_id\": \"e367da67-c468-406c-b731-c6cd56843004\", \"name\": \"Prepare Data\", \"status\": \"Failed\", \"start_time\": \"2023-03-23T10:06:31.805515Z\", \"created_time\": \"2023-03-23T10:06:20.372822Z\", \"end_time\": \"2023-03-23T10:06:58.113305Z\", \"duration\": \"0:00:37\", \"run_number\": 1679565980, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2023-03-23T10:06:20.372822Z\", \"is_reused\": \"\"}, {\"run_id\": \"\", \"name\": \"Train and Register Model\", \"status\": \"NotStarted\", \"start_time\": \"\", \"created_time\": \"\", \"end_time\": \"\", \"duration\": \"\"}]}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.49.0\"}, \"loading\": false}"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "PipelineRunId: 4d0b4e07-0eba-4770-bcf0-6a8b7ef331f2\nLink to Azure Machine Learning Portal: https://ml.azure.com/runs/4d0b4e07-0eba-4770-bcf0-6a8b7ef331f2?wsid=/subscriptions/7c248226-48dc-4d36-baa0-0f0883669328/resourcegroups/saurv_01/workspaces/saurav_aml&tid=0563aea9-b886-4b62-9457-a85924a13bd1\nPipelineRun Status: NotStarted\nPipelineRun Status: Running\n\n\nStepRunId: e367da67-c468-406c-b731-c6cd56843004\nLink to Azure Machine Learning Portal: https://ml.azure.com/runs/e367da67-c468-406c-b731-c6cd56843004?wsid=/subscriptions/7c248226-48dc-4d36-baa0-0f0883669328/resourcegroups/saurv_01/workspaces/saurav_aml&tid=0563aea9-b886-4b62-9457-a85924a13bd1\nStepRun( Prepare Data ) Status: NotStarted\nStepRun( Prepare Data ) Status: Running\n\nStepRun(Prepare Data) Execution Summary\n========================================\nStepRun( Prepare Data ) Status: Failed\n\nWarnings:\nAzureMLCompute job failed.\nExecutionFailed: [REDACTED]\n\texit_codes: 1\n\tAppinsights Reachable: Some(true)\nExecution failed. User process '/azureml-envs/azureml_95e68210fe9605c8784b26066c59b46a/bin/python' exited with status code 1. Please check log file 'user_logs/std_log.txt' for error details. Error:     return quantile_with_mask(values, mask, fill_value, q\n"
        },
        {
          "output_type": "error",
          "ename": "ActivityFailedException",
          "evalue": "ActivityFailedException:\n\tMessage: Activity Failed:\n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Execution failed. User process '/azureml-envs/azureml_95e68210fe9605c8784b26066c59b46a/bin/python' exited with status code 1. Please check log file 'user_logs/std_log.txt' for error details. Error:     return quantile_with_mask(values, mask, fill_value, qs, interpolation)\\n  File \\\"/azureml-envs/azureml_95e68210fe9605c8784b26066c59b46a/lib/python3.8/site-packages/pandas/core/array_algos/quantile.py\\\", line 95, in quantile_with_mask\\n    result = _nanpercentile(\\n  File \\\"/azureml-envs/azureml_95e68210fe9605c8784b26066c59b46a/lib/python3.8/site-packages/pandas/core/array_algos/quantile.py\\\", line 216, in _nanpercentile\\n    return np.percentile(\\n  File \\\"<__array_function__ internals>\\\", line 180, in percentile\\n  File \\\"/azureml-envs/azureml_95e68210fe9605c8784b26066c59b46a/lib/python3.8/site-packages/numpy/lib/function_base.py\\\", line 4166, in percentile\\n    return _quantile_unchecked(\\n  File \\\"/azureml-envs/azureml_95e68210fe9605c8784b26066c59b46a/lib/python3.8/site-packages/numpy/lib/function_base.py\\\", line 4424, in _quantile_unchecked\\n    r, k = _ureduce(a,\\n  File \\\"/azureml-envs/azureml_95e68210fe9605c8784b26066c59b46a/lib/python3.8/site-packages/numpy/lib/function_base.py\\\", line 3725, in _ureduce\\n    r = func(a, **kwargs)\\n  File \\\"/azureml-envs/azureml_95e68210fe9605c8784b26066c59b46a/lib/python3.8/site-packages/numpy/lib/function_base.py\\\", line 4593, in _quantile_ureduce_func\\n    result = _quantile(arr,\\n  File \\\"/azureml-envs/azureml_95e68210fe9605c8784b26066c59b46a/lib/python3.8/site-packages/numpy/lib/function_base.py\\\", line 4710, in _quantile\\n    result = _lerp(previous,\\n  File \\\"/azureml-envs/azureml_95e68210fe9605c8784b26066c59b46a/lib/python3.8/site-packages/numpy/lib/function_base.py\\\", line 4527, in _lerp\\n    diff_b_a = subtract(b, a)\\nTypeError: numpy boolean subtract, the `-` operator, is not supported, use the bitwise_xor, the `^` operator, or the logical_xor function instead.\\n\\n\",\n        \"messageParameters\": {},\n        \"details\": []\n    },\n    \"time\": \"0001-01-01T00:00:00.000Z\",\n    \"componentName\": \"CommonRuntime\"\n}\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Activity Failed:\\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"message\\\": \\\"Execution failed. User process '/azureml-envs/azureml_95e68210fe9605c8784b26066c59b46a/bin/python' exited with status code 1. Please check log file 'user_logs/std_log.txt' for error details. Error:     return quantile_with_mask(values, mask, fill_value, qs, interpolation)\\\\n  File \\\\\\\"/azureml-envs/azureml_95e68210fe9605c8784b26066c59b46a/lib/python3.8/site-packages/pandas/core/array_algos/quantile.py\\\\\\\", line 95, in quantile_with_mask\\\\n    result = _nanpercentile(\\\\n  File \\\\\\\"/azureml-envs/azureml_95e68210fe9605c8784b26066c59b46a/lib/python3.8/site-packages/pandas/core/array_algos/quantile.py\\\\\\\", line 216, in _nanpercentile\\\\n    return np.percentile(\\\\n  File \\\\\\\"<__array_function__ internals>\\\\\\\", line 180, in percentile\\\\n  File \\\\\\\"/azureml-envs/azureml_95e68210fe9605c8784b26066c59b46a/lib/python3.8/site-packages/numpy/lib/function_base.py\\\\\\\", line 4166, in percentile\\\\n    return _quantile_unchecked(\\\\n  File \\\\\\\"/azureml-envs/azureml_95e68210fe9605c8784b26066c59b46a/lib/python3.8/site-packages/numpy/lib/function_base.py\\\\\\\", line 4424, in _quantile_unchecked\\\\n    r, k = _ureduce(a,\\\\n  File \\\\\\\"/azureml-envs/azureml_95e68210fe9605c8784b26066c59b46a/lib/python3.8/site-packages/numpy/lib/function_base.py\\\\\\\", line 3725, in _ureduce\\\\n    r = func(a, **kwargs)\\\\n  File \\\\\\\"/azureml-envs/azureml_95e68210fe9605c8784b26066c59b46a/lib/python3.8/site-packages/numpy/lib/function_base.py\\\\\\\", line 4593, in _quantile_ureduce_func\\\\n    result = _quantile(arr,\\\\n  File \\\\\\\"/azureml-envs/azureml_95e68210fe9605c8784b26066c59b46a/lib/python3.8/site-packages/numpy/lib/function_base.py\\\\\\\", line 4710, in _quantile\\\\n    result = _lerp(previous,\\\\n  File \\\\\\\"/azureml-envs/azureml_95e68210fe9605c8784b26066c59b46a/lib/python3.8/site-packages/numpy/lib/function_base.py\\\\\\\", line 4527, in _lerp\\\\n    diff_b_a = subtract(b, a)\\\\nTypeError: numpy boolean subtract, the `-` operator, is not supported, use the bitwise_xor, the `^` operator, or the logical_xor function instead.\\\\n\\\\n\\\",\\n        \\\"messageParameters\\\": {},\\n        \\\"details\\\": []\\n    },\\n    \\\"time\\\": \\\"0001-01-01T00:00:00.000Z\\\",\\n    \\\"componentName\\\": \\\"CommonRuntime\\\"\\n}\"\n    }\n}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mActivityFailedException\u001b[0m                   Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[159], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline submitted for execution.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m RunDetails(pipeline_run)\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m---> 15\u001b[0m \u001b[43mpipeline_run\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_for_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshow_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azureml/pipeline/core/run.py:295\u001b[0m, in \u001b[0;36mPipelineRun.wait_for_completion\u001b[0;34m(self, show_output, timeout_seconds, raise_on_error)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 295\u001b[0m     \u001b[43mstep_run\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_for_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout_seconds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_seconds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtime_elapsed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mraise_on_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraise_on_error\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;66;03m# If there are package conflicts in the user's environment, the run rehydration\u001b[39;00m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;66;03m# will not work and we will receive a Run object instead of StepRun.\u001b[39;00m\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;66;03m# Run.wait_for_completion() does not have a parameter timeout_seconds, which\u001b[39;00m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;66;03m# will generate a TypeError here.  As a workaround, call the method without\u001b[39;00m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;66;03m# this parameter.\u001b[39;00m\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(step_run, StepRun):\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azureml/pipeline/core/run.py:738\u001b[0m, in \u001b[0;36mStepRun.wait_for_completion\u001b[0;34m(self, show_output, timeout_seconds, raise_on_error)\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m show_output:\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 738\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream_run_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout_seconds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_seconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mraise_on_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraise_on_error\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    740\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m    741\u001b[0m         error_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe output streaming for the run interrupted.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m    742\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBut the run is still executing on the compute target. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m    743\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetails for canceling the run can be found here: \u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m    744\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://aka.ms/aml-docs-cancel-run\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azureml/pipeline/core/run.py:831\u001b[0m, in \u001b[0;36mStepRun._stream_run_output\u001b[0;34m(self, timeout_seconds, raise_on_error)\u001b[0m\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28mprint\u001b[39m(json\u001b[38;5;241m.\u001b[39mdumps(error, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error \u001b[38;5;129;01mand\u001b[39;00m raise_on_error:\n\u001b[0;32m--> 831\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ActivityFailedException(error_details\u001b[38;5;241m=\u001b[39mjson\u001b[38;5;241m.\u001b[39mdumps(error, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m    833\u001b[0m \u001b[38;5;28mprint\u001b[39m(final_details)\n\u001b[1;32m    834\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[0;31mActivityFailedException\u001b[0m: ActivityFailedException:\n\tMessage: Activity Failed:\n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Execution failed. User process '/azureml-envs/azureml_95e68210fe9605c8784b26066c59b46a/bin/python' exited with status code 1. Please check log file 'user_logs/std_log.txt' for error details. Error:     return quantile_with_mask(values, mask, fill_value, qs, interpolation)\\n  File \\\"/azureml-envs/azureml_95e68210fe9605c8784b26066c59b46a/lib/python3.8/site-packages/pandas/core/array_algos/quantile.py\\\", line 95, in quantile_with_mask\\n    result = _nanpercentile(\\n  File \\\"/azureml-envs/azureml_95e68210fe9605c8784b26066c59b46a/lib/python3.8/site-packages/pandas/core/array_algos/quantile.py\\\", line 216, in _nanpercentile\\n    return np.percentile(\\n  File \\\"<__array_function__ internals>\\\", line 180, in percentile\\n  File \\\"/azureml-envs/azureml_95e68210fe9605c8784b26066c59b46a/lib/python3.8/site-packages/numpy/lib/function_base.py\\\", line 4166, in percentile\\n    return _quantile_unchecked(\\n  File \\\"/azureml-envs/azureml_95e68210fe9605c8784b26066c59b46a/lib/python3.8/site-packages/numpy/lib/function_base.py\\\", line 4424, in _quantile_unchecked\\n    r, k = _ureduce(a,\\n  File \\\"/azureml-envs/azureml_95e68210fe9605c8784b26066c59b46a/lib/python3.8/site-packages/numpy/lib/function_base.py\\\", line 3725, in _ureduce\\n    r = func(a, **kwargs)\\n  File \\\"/azureml-envs/azureml_95e68210fe9605c8784b26066c59b46a/lib/python3.8/site-packages/numpy/lib/function_base.py\\\", line 4593, in _quantile_ureduce_func\\n    result = _quantile(arr,\\n  File \\\"/azureml-envs/azureml_95e68210fe9605c8784b26066c59b46a/lib/python3.8/site-packages/numpy/lib/function_base.py\\\", line 4710, in _quantile\\n    result = _lerp(previous,\\n  File \\\"/azureml-envs/azureml_95e68210fe9605c8784b26066c59b46a/lib/python3.8/site-packages/numpy/lib/function_base.py\\\", line 4527, in _lerp\\n    diff_b_a = subtract(b, a)\\nTypeError: numpy boolean subtract, the `-` operator, is not supported, use the bitwise_xor, the `^` operator, or the logical_xor function instead.\\n\\n\",\n        \"messageParameters\": {},\n        \"details\": []\n    },\n    \"time\": \"0001-01-01T00:00:00.000Z\",\n    \"componentName\": \"CommonRuntime\"\n}\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Activity Failed:\\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"message\\\": \\\"Execution failed. User process '/azureml-envs/azureml_95e68210fe9605c8784b26066c59b46a/bin/python' exited with status code 1. Please check log file 'user_logs/std_log.txt' for error details. Error:     return quantile_with_mask(values, mask, fill_value, qs, interpolation)\\\\n  File \\\\\\\"/azureml-envs/azureml_95e68210fe9605c8784b26066c59b46a/lib/python3.8/site-packages/pandas/core/array_algos/quantile.py\\\\\\\", line 95, in quantile_with_mask\\\\n    result = _nanpercentile(\\\\n  File \\\\\\\"/azureml-envs/azureml_95e68210fe9605c8784b26066c59b46a/lib/python3.8/site-packages/pandas/core/array_algos/quantile.py\\\\\\\", line 216, in _nanpercentile\\\\n    return np.percentile(\\\\n  File \\\\\\\"<__array_function__ internals>\\\\\\\", line 180, in percentile\\\\n  File \\\\\\\"/azureml-envs/azureml_95e68210fe9605c8784b26066c59b46a/lib/python3.8/site-packages/numpy/lib/function_base.py\\\\\\\", line 4166, in percentile\\\\n    return _quantile_unchecked(\\\\n  File \\\\\\\"/azureml-envs/azureml_95e68210fe9605c8784b26066c59b46a/lib/python3.8/site-packages/numpy/lib/function_base.py\\\\\\\", line 4424, in _quantile_unchecked\\\\n    r, k = _ureduce(a,\\\\n  File \\\\\\\"/azureml-envs/azureml_95e68210fe9605c8784b26066c59b46a/lib/python3.8/site-packages/numpy/lib/function_base.py\\\\\\\", line 3725, in _ureduce\\\\n    r = func(a, **kwargs)\\\\n  File \\\\\\\"/azureml-envs/azureml_95e68210fe9605c8784b26066c59b46a/lib/python3.8/site-packages/numpy/lib/function_base.py\\\\\\\", line 4593, in _quantile_ureduce_func\\\\n    result = _quantile(arr,\\\\n  File \\\\\\\"/azureml-envs/azureml_95e68210fe9605c8784b26066c59b46a/lib/python3.8/site-packages/numpy/lib/function_base.py\\\\\\\", line 4710, in _quantile\\\\n    result = _lerp(previous,\\\\n  File \\\\\\\"/azureml-envs/azureml_95e68210fe9605c8784b26066c59b46a/lib/python3.8/site-packages/numpy/lib/function_base.py\\\\\\\", line 4527, in _lerp\\\\n    diff_b_a = subtract(b, a)\\\\nTypeError: numpy boolean subtract, the `-` operator, is not supported, use the bitwise_xor, the `^` operator, or the logical_xor function instead.\\\\n\\\\n\\\",\\n        \\\"messageParameters\\\": {},\\n        \\\"details\\\": []\\n    },\\n    \\\"time\\\": \\\"0001-01-01T00:00:00.000Z\\\",\\n    \\\"componentName\\\": \\\"CommonRuntime\\\"\\n}\"\n    }\n}"
          ]
        }
      ],
      "execution_count": 159,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1679566554547
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for run in pipeline_run.get_children():\r\n",
        "    print(run.name, ':')\r\n",
        "    metrics = run.get_metrics()\r\n",
        "    for metric_name in metrics:\r\n",
        "        print('\\t',metric_name, \":\", metrics[metric_name])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1679566554841
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Model\r\n",
        "\r\n",
        "for model in Model.list(ws):\r\n",
        "    print(model.name, 'version:', model.version)\r\n",
        "    for tag_name in model.tags:\r\n",
        "        tag = model.tags[tag_name]\r\n",
        "        print ('\\t',tag_name, ':', tag)\r\n",
        "    for prop_name in model.properties:\r\n",
        "        prop = model.properties[prop_name]\r\n",
        "        print ('\\t',prop_name, ':', prop)\r\n",
        "    print('\\n')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1679566554892
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}