{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\r\n",
        "import azureml.core\r\n",
        "from azureml.core import Workspace\r\n",
        "\r\n",
        "# Load the workspace from the saved config file\r\n",
        "ws = Workspace.from_config()\r\n",
        "print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Ready to use Azure ML 1.49.0 to work with saurav_aml\n"
        }
      ],
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1679558827607
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Dataset\r\n",
        "from azureml.data.datapath import DataPath\r\n",
        "\r\n",
        "default_ds = ws.get_default_datastore()\r\n",
        "\r\n",
        "if 'insurance dataset' not in ws.datasets:\r\n",
        "    Dataset.File.upload_directory(src_dir='data',\r\n",
        "                              target=DataPath(default_ds, 'insurance-data/')\r\n",
        "                              )\r\n",
        "\r\n",
        "    #Create a tabular dataset from the path on the datastore (this may take a short while)\r\n",
        "    tab_data_set = Dataset.Tabular.from_delimited_files(path=(default_ds, 'insurance-data/*.csv'))\r\n",
        "\r\n",
        "    # Register the tabular dataset\r\n",
        "    try:\r\n",
        "        tab_data_set = tab_data_set.register(workspace=ws, \r\n",
        "                                name='insurance dataset',\r\n",
        "                                description='insurance data',\r\n",
        "                                tags = {'format':'CSV'},\r\n",
        "                                create_new_version=True)\r\n",
        "        print('Dataset registered.')\r\n",
        "    except Exception as ex:\r\n",
        "        print(ex)\r\n",
        "else:\r\n",
        "    print('Dataset already registered.')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Dataset already registered.\n"
        }
      ],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1679558828339
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\r\n",
        "# Create a folder for the pipeline step files\r\n",
        "experiment_folder = 'insurance_pipeline'\r\n",
        "os.makedirs(experiment_folder, exist_ok=True)\r\n",
        "\r\n",
        "print(experiment_folder)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "insurance_pipeline\n"
        }
      ],
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1679558828784
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $experiment_folder/prep_insurance.py\r\n",
        "# Import libraries\r\n",
        "import os\r\n",
        "import argparse\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from azureml.core import Run\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "\r\n",
        "# Get parameters\r\n",
        "parser = argparse.ArgumentParser()\r\n",
        "parser.add_argument(\"--input-data\", type=str, dest='raw_dataset_id', help='raw dataset')\r\n",
        "parser.add_argument('--prepped-data', type=str, dest='prepped_data', default='prepped_data', help='Folder for results')\r\n",
        "args = parser.parse_args()\r\n",
        "prep_folder = args.prepped_data\r\n",
        "\r\n",
        "# Get the experiment run context\r\n",
        "run = Run.get_context()\r\n",
        "\r\n",
        "# load the data (passed as an input dataset)\r\n",
        "print(\"Loading Data...\")\r\n",
        "df = run.input_datasets['raw_data'].to_pandas_dataframe()\r\n",
        "\r\n",
        "# Log raw row count\r\n",
        "row_count = (len(df))\r\n",
        "run.log('raw_rows', row_count)\r\n",
        "\r\n",
        "def iqr_outlier_cap(df,column):\r\n",
        "    q1 = df[column].quantile(0.25)\r\n",
        "    q2 = df[column].quantile(0.75)\r\n",
        "    #finding out the value of Inter Quartile Range\r\n",
        "    IQR = q2 - q1\r\n",
        "    #defining max and min limits\r\n",
        "    max_limit = q2 + (1.5 * IQR)\r\n",
        "    min_limit = q1 - (1.5 * IQR) \r\n",
        "    #capping\r\n",
        "    df_new = pd.DataFrame(np.where(df[column] > max_limit, max_limit, \r\n",
        "             (np.where(df[column] < min_limit, min_limit, df[column]))), columns=[column])\r\n",
        "    return df_new\r\n",
        "\r\n",
        "def Preprocessing(df):\r\n",
        "    \"\"\"Data Pre-processing\"\"\"\r\n",
        "    # if '?' in the datset which we have to remove by NaN Values\r\n",
        "    df = df.replace('?',np.NaN)\r\n",
        "\r\n",
        "    df['collision_type'].fillna(df['collision_type'].mode()[0], inplace = True)\r\n",
        "    df['property_damage'].fillna('NO', inplace = True)\r\n",
        "    df['police_report_available'].fillna('NO', inplace = True)\r\n",
        "\r\n",
        "    # let's extrat days, month and year from policy bind date\r\n",
        "    df['policy_bind_date'] = pd.to_datetime(df['policy_bind_date'], errors = 'coerce')\r\n",
        "\r\n",
        "    # dropping unimportant columns\r\n",
        "    df = df.drop(columns = [\r\n",
        "        'policy_number', \r\n",
        "        'insured_zip',  \r\n",
        "        'incident_date', \r\n",
        "        'umbrella_limit', \r\n",
        "        '_c39'])\r\n",
        "\r\n",
        "    numeric_data = df._get_numeric_data()\r\n",
        "    cat_data = df.select_dtypes(include=['object'])\r\n",
        "\r\n",
        "    lst=[]\r\n",
        "    for i in numeric_data.columns:\r\n",
        "        lst.append(iqr_outlier_cap(numeric_data,i))\r\n",
        "    numeric_data_cap=pd.concat(lst,axis=1)\r\n",
        "\r\n",
        "    for c in cat_data:\r\n",
        "        lbl = LabelEncoder()\r\n",
        "        lbl.fit(cat_data[c].values)\r\n",
        "        cat_data[c] = lbl.transform(cat_data[c].values)\r\n",
        "\r\n",
        "\r\n",
        "    # Normalize the numeric columns\r\n",
        "    scaler = MinMaxScaler()\r\n",
        "\r\n",
        "    # num_data_clean = scaler.fit_transform(numeric_data_cap)\r\n",
        "\r\n",
        "    clean_data = pd.concat([numeric_data_cap,cat_data],axis=1)\r\n",
        "\r\n",
        "    clean_data = scaler.fit_transform(clean_data)\r\n",
        "    \r\n",
        "    \r\n",
        "    return clean_data\r\n",
        "\r\n",
        "dataPrep = Preprocessing(df)\r\n",
        "\r\n",
        "# # Log processed rows\r\n",
        "row_count = (len(df))\r\n",
        "run.log('processed_rows', row_count)\r\n",
        "\r\n",
        "# Save the prepped data\r\n",
        "print(\"Saving Data...\")\r\n",
        "os.makedirs(prep_folder, exist_ok=True)\r\n",
        "save_path_1 = os.path.join(prep_folder,'prep_data.csv')\r\n",
        "dataPrep.to_csv(save_path_1, index=False, header=True)\r\n",
        "\r\n",
        "# End the run\r\n",
        "run.complete()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting insurance_pipeline/prep_insurance.py\n"
        }
      ],
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $experiment_folder/train_insurance.py\r\n",
        "# Import libraries\r\n",
        "from azureml.core import Run, Model\r\n",
        "import argparse\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import joblib\r\n",
        "import os\r\n",
        "# from azureml.core import Run\r\n",
        "import argparse, joblib, os\r\n",
        "import argparse\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from sklearn.metrics import accuracy_score, recall_score, classification_report, cohen_kappa_score\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "# from sklearn.tree import RandomForestClassifier\r\n",
        "from sklearn.metrics import roc_auc_score\r\n",
        "from sklearn.metrics import roc_curve\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "# Get parameters\r\n",
        "parser = argparse.ArgumentParser()\r\n",
        "parser.add_argument(\"--training-data\", type=str, dest='training_data', help='training data')\r\n",
        "args = parser.parse_args()\r\n",
        "training_data = args.training_data\r\n",
        "\r\n",
        "# Get the experiment run context\r\n",
        "run = Run.get_context()\r\n",
        "\r\n",
        "# load the prepared data file in the training folder\r\n",
        "print(\"Loading Data...\")\r\n",
        "file_path = os.path.join(training_data,'prep_data.csv')\r\n",
        "data_prep = pd.read_csv(file_path)\r\n",
        "\r\n",
        "\r\n",
        "# Get parameters\r\n",
        "import argparse\r\n",
        "parser = argparse.ArgumentParser()\r\n",
        "parser.add_argument(\"--n_estimators\", type=int)\r\n",
        "parser.add_argument(\"--min_samples_leaf\", type=int)\r\n",
        "parser.add_argument(\"--datafolder\", type=str)\r\n",
        "\r\n",
        "args, unknown = parser.parse_known_args()\r\n",
        "\r\n",
        "ne = args.n_estimators\r\n",
        "msl = args.min_samples_leaf\r\n",
        "\r\n",
        "print(ne, msl)\r\n",
        "\r\n",
        "X = data_prep.iloc[:, 0:-1]\r\n",
        "\r\n",
        "y = data_prep.iloc[:, -1]\r\n",
        "\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=1234)\r\n",
        "\r\n",
        "# Baseline Random forest based Model\r\n",
        "# rfc = RandomForestClassifier(n_estimators=ne, min_samples_leaf=msl)\r\n",
        "rfc = RandomForestClassifier()\r\n",
        "\r\n",
        "rfcg = rfc.fit(X_train, y_train) # fit on training data\r\n",
        "Y_predict = rfcg.predict(X_test)\r\n",
        "\r\n",
        "# Get the probability score - Scored Probabilities\r\n",
        "Y_prob = rfcg.predict_proba(X_test)[:, 1]\r\n",
        "\r\n",
        "# Get Confusion matrix and the accuracy/score - Evaluate\r\n",
        "\r\n",
        "cm    = confusion_matrix(y_test, Y_predict)\r\n",
        "accuracy = accuracy_score(y_test, Y_predict)\r\n",
        "\r\n",
        "# Create the confusion matrix dictionary\r\n",
        "cm_dict = {\"schema_type\": \"confusion_matrix\",\r\n",
        "           \"schema_version\": \"v1\",\r\n",
        "           \"data\": {\"class_labels\": [\"N\", \"Y\"],\r\n",
        "                    \"matrix\": cm.tolist()}\r\n",
        "           }\r\n",
        "\r\n",
        "run.log(\"TotalObservations\", len(data_prep))\r\n",
        "run.log_confusion_matrix(\"ConfusionMatrix\", cm_dict)\r\n",
        "run.log(\"Accuracy\", accuracy)\r\n",
        "\r\n",
        "# # Save the model in the run outputs\r\n",
        "# os.makedirs('outputs', exist_ok=True)\r\n",
        "# joblib.dump(value=rfc, filename='outputs/insurance_model.pkl')\r\n",
        "\r\n",
        "# # Complete the run\r\n",
        "# run.complete()\r\n",
        "\r\n",
        "# Save the trained model in the outputs folder\r\n",
        "print(\"Saving model...\")\r\n",
        "os.makedirs('outputs', exist_ok=True)\r\n",
        "model_file = os.path.join('outputs', 'insurance_model.pkl')\r\n",
        "joblib.dump(value=rfc, filename=model_file)\r\n",
        "\r\n",
        "# Register the model\r\n",
        "print('Registering model...')\r\n",
        "Model.register(workspace=run.experiment.workspace,\r\n",
        "               model_path = model_file,\r\n",
        "               model_name = 'insurance_model',\r\n",
        "               tags={'Training context':'Pipeline'},\r\n",
        "               properties={'Accuracy': np.float(accuracy)})\r\n",
        "\r\n",
        "\r\n",
        "run.complete()\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting insurance_pipeline/train_insurance.py\n"
        }
      ],
      "execution_count": 17,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.compute import ComputeTarget, AmlCompute\r\n",
        "from azureml.core.compute_target import ComputeTargetException\r\n",
        "\r\n",
        "cluster_name = \"saurav-compute-cluster\"\r\n",
        "\r\n",
        "try:\r\n",
        "    # Check for existing compute target\r\n",
        "    pipeline_cluster = ComputeTarget(workspace=ws, name=cluster_name)\r\n",
        "    print('Found existing cluster, use it.')\r\n",
        "except ComputeTargetException:\r\n",
        "    # If it doesn't already exist, create it\r\n",
        "    try:\r\n",
        "        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2', max_nodes=2)\r\n",
        "        pipeline_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\r\n",
        "        pipeline_cluster.wait_for_completion(show_output=True)\r\n",
        "    except Exception as ex:\r\n",
        "        print(ex)\r\n",
        "    "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Found existing cluster, use it.\n"
        }
      ],
      "execution_count": 18,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1679558830691
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $experiment_folder/experiment_new_env.yml\r\n",
        "name: experiment_env\r\n",
        "dependencies:\r\n",
        "- python=3.8\r\n",
        "- scikit-learn\r\n",
        "- ipykernel\r\n",
        "- matplotlib\r\n",
        "- pandas\r\n",
        "- pip\r\n",
        "- pip:\r\n",
        "  - azureml-defaults\r\n",
        "  - pyarrow"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting insurance_pipeline/experiment_new_env.yml\n"
        }
      ],
      "execution_count": 19,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\r\n",
        "from azureml.core import Environment\r\n",
        "from azureml.core.runconfig import RunConfiguration\r\n",
        "\r\n",
        "# Create a Python environment for the experiment (from a .yml file)\r\n",
        "experiment_env = Environment.from_conda_specification(\"experiment_env\", experiment_folder + \"/experiment_new_env.yml\")\r\n",
        "\r\n",
        "# Register the environment \r\n",
        "experiment_env.register(workspace=ws)\r\n",
        "registered_env = Environment.get(ws, 'experiment_env')\r\n",
        "\r\n",
        "# Create a new runconfig object for the pipeline\r\n",
        "pipeline_run_config = RunConfiguration()\r\n",
        "\r\n",
        "# Use the compute you created above. \r\n",
        "pipeline_run_config.target = pipeline_cluster\r\n",
        "\r\n",
        "# Assign the environment to the run configuration\r\n",
        "pipeline_run_config.environment = registered_env\r\n",
        "\r\n",
        "print (\"Run configuration created.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Run configuration created.\n"
        }
      ],
      "execution_count": 20,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1679558832197
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.data import OutputFileDatasetConfig\r\n",
        "from azureml.pipeline.steps import PythonScriptStep\r\n",
        "\r\n",
        "# Get the training dataset\r\n",
        "insurance_ds = ws.datasets.get(\"insurance dataset\")\r\n",
        "\r\n",
        "# Create an OutputFileDatasetConfig (temporary Data Reference) for data passed from step 1 to step 2\r\n",
        "prepped_data = OutputFileDatasetConfig(\"prepped_data\")\r\n",
        "\r\n",
        "# Step 1, Run the data prep script\r\n",
        "prep_step = PythonScriptStep(name = \"Prepare Data\",\r\n",
        "                                source_directory = experiment_folder,\r\n",
        "                                script_name = \"prep_insurance.py\",\r\n",
        "                                arguments = ['--input-data', insurance_ds.as_named_input('raw_data'),\r\n",
        "                                             '--prepped-data', prepped_data],\r\n",
        "                                compute_target = pipeline_cluster,\r\n",
        "                                runconfig = pipeline_run_config,\r\n",
        "                                allow_reuse = True)\r\n",
        "\r\n",
        "# Step 2, run the training script\r\n",
        "train_step = PythonScriptStep(name = \"Train and Register Model\",\r\n",
        "                                source_directory = experiment_folder,\r\n",
        "                                script_name = \"train_insurance.py\",\r\n",
        "                                arguments = ['--training-data', prepped_data.as_input()],\r\n",
        "                                compute_target = pipeline_cluster,\r\n",
        "                                runconfig = pipeline_run_config,\r\n",
        "                                allow_reuse = True)\r\n",
        "\r\n",
        "print(\"Pipeline steps defined\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Pipeline steps defined\n"
        }
      ],
      "execution_count": 21,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1679558832819
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Experiment\r\n",
        "from azureml.pipeline.core import Pipeline\r\n",
        "from azureml.widgets import RunDetails\r\n",
        "\r\n",
        "# Construct the pipeline\r\n",
        "pipeline_steps = [prep_step, train_step]\r\n",
        "pipeline_new = Pipeline(workspace=ws, steps=pipeline_steps)\r\n",
        "print(\"Pipeline is built.\")\r\n",
        "\r\n",
        "# Create an experiment and run the pipeline\r\n",
        "experiment_new = Experiment(workspace=ws, name = 'saurav-insurance-pipeline')\r\n",
        "pipeline_run = experiment_new.submit(pipeline_new, regenerate_outputs=True)\r\n",
        "print(\"Pipeline submitted for execution.\")\r\n",
        "RunDetails(pipeline_run).show()\r\n",
        "pipeline_run.wait_for_completion(show_output=True)\r\n",
        "     "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Pipeline is built.\nCreated step Prepare Data [b0f7254e][5975af41-b0dd-4aca-b98a-6451234f3acd], (This step will run and generate new outputs)\nCreated step Train and Register Model [de5d27d1][0b95a33d-a49c-4816-b3da-3db1c82c6413], (This step will run and generate new outputs)\nSubmitted PipelineRun 1b4b65a1-19d4-4943-8c5c-f2d5e3f73f2a\nLink to Azure Machine Learning Portal: https://ml.azure.com/runs/1b4b65a1-19d4-4943-8c5c-f2d5e3f73f2a?wsid=/subscriptions/7c248226-48dc-4d36-baa0-0f0883669328/resourcegroups/saurv_01/workspaces/saurav_aml&tid=0563aea9-b886-4b62-9457-a85924a13bd1\nPipeline submitted for execution.\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "_PipelineWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', …",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "59b8c3ca94824ad49abb94e1bc6c2173"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/aml.mini.widget.v1": "{\"status\": \"Running\", \"workbench_run_details_uri\": \"https://ml.azure.com/runs/1b4b65a1-19d4-4943-8c5c-f2d5e3f73f2a?wsid=/subscriptions/7c248226-48dc-4d36-baa0-0f0883669328/resourcegroups/saurv_01/workspaces/saurav_aml&tid=0563aea9-b886-4b62-9457-a85924a13bd1\", \"run_id\": \"1b4b65a1-19d4-4943-8c5c-f2d5e3f73f2a\", \"run_properties\": {\"run_id\": \"1b4b65a1-19d4-4943-8c5c-f2d5e3f73f2a\", \"created_utc\": \"2023-03-23T07:58:16.924107Z\", \"properties\": {\"azureml.runsource\": \"azureml.PipelineRun\", \"runSource\": \"SDK\", \"runType\": \"SDK\", \"azureml.parameters\": \"{}\", \"azureml.continue_on_step_failure\": \"False\", \"azureml.continue_on_failed_optional_input\": \"True\", \"azureml.pipelineComponent\": \"pipelinerun\"}, \"tags\": {}, \"end_time_utc\": null, \"status\": \"Running\", \"log_files\": {\"logs/azureml/executionlogs.txt\": \"https://sauravaml7390822349.blob.core.windows.net/azureml/ExperimentRun/dcid.1b4b65a1-19d4-4943-8c5c-f2d5e3f73f2a/logs/azureml/executionlogs.txt?sv=2019-07-07&sr=b&sig=VJK2SHn1Cuv80CeoiQ9dAq21QgE50pKbxnGqorJJbog%3D&skoid=d4a5ccdf-0157-4f3b-b8b0-16434308cc74&sktid=0563aea9-b886-4b62-9457-a85924a13bd1&skt=2023-03-23T07%3A06%3A05Z&ske=2023-03-24T15%3A16%3A05Z&sks=b&skv=2019-07-07&st=2023-03-23T07%3A48%3A39Z&se=2023-03-23T15%3A58%3A39Z&sp=r\", \"logs/azureml/stderrlogs.txt\": \"https://sauravaml7390822349.blob.core.windows.net/azureml/ExperimentRun/dcid.1b4b65a1-19d4-4943-8c5c-f2d5e3f73f2a/logs/azureml/stderrlogs.txt?sv=2019-07-07&sr=b&sig=c7Q0BMJ2j5Vi8ZeNONQ%2FP6WdN6iElc%2Bd8Yro%2BVzalBY%3D&skoid=d4a5ccdf-0157-4f3b-b8b0-16434308cc74&sktid=0563aea9-b886-4b62-9457-a85924a13bd1&skt=2023-03-23T07%3A06%3A05Z&ske=2023-03-24T15%3A16%3A05Z&sks=b&skv=2019-07-07&st=2023-03-23T07%3A48%3A39Z&se=2023-03-23T15%3A58%3A39Z&sp=r\", \"logs/azureml/stdoutlogs.txt\": \"https://sauravaml7390822349.blob.core.windows.net/azureml/ExperimentRun/dcid.1b4b65a1-19d4-4943-8c5c-f2d5e3f73f2a/logs/azureml/stdoutlogs.txt?sv=2019-07-07&sr=b&sig=X2WT5t5N6m4QB1ccG0SgTXVP2DobOncOdBrc08l1ow0%3D&skoid=d4a5ccdf-0157-4f3b-b8b0-16434308cc74&sktid=0563aea9-b886-4b62-9457-a85924a13bd1&skt=2023-03-23T07%3A06%3A05Z&ske=2023-03-24T15%3A16%3A05Z&sks=b&skv=2019-07-07&st=2023-03-23T07%3A48%3A39Z&se=2023-03-23T15%3A58%3A39Z&sp=r\"}, \"log_groups\": [[\"logs/azureml/executionlogs.txt\", \"logs/azureml/stderrlogs.txt\", \"logs/azureml/stdoutlogs.txt\"]], \"run_duration\": \"0:01:04\", \"run_number\": \"1679558296\", \"run_queued_details\": {\"status\": \"Running\", \"details\": null}}, \"child_runs\": [{\"run_id\": \"9990b5aa-0b00-41e0-b1a8-7d8290c80443\", \"name\": \"Prepare Data\", \"status\": \"Running\", \"start_time\": \"\", \"created_time\": \"2023-03-23T07:58:19.560988Z\", \"end_time\": \"\", \"duration\": \"0:01:02\", \"run_number\": 1679558299, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2023-03-23T07:58:19.560988Z\", \"is_reused\": \"\"}, {\"run_id\": \"\", \"name\": \"Train and Register Model\", \"status\": \"NotStarted\", \"start_time\": \"\", \"created_time\": \"\", \"end_time\": \"\", \"duration\": \"\"}], \"children_metrics\": {\"categories\": null, \"series\": null, \"metricName\": null}, \"run_metrics\": [], \"run_logs\": \"[2023-03-23 07:58:18Z] Submitting 1 runs, first five are: b0f7254e:9990b5aa-0b00-41e0-b1a8-7d8290c80443\\n\", \"graph\": {\"datasource_nodes\": {\"6fbda1d8\": {\"node_id\": \"6fbda1d8\", \"name\": \"insurance dataset\"}}, \"module_nodes\": {\"b0f7254e\": {\"node_id\": \"b0f7254e\", \"name\": \"Prepare Data\", \"status\": \"Running\", \"_is_reused\": false, \"run_id\": \"9990b5aa-0b00-41e0-b1a8-7d8290c80443\"}, \"de5d27d1\": {\"node_id\": \"de5d27d1\", \"name\": \"Train and Register Model\", \"status\": \"NotStarted\"}}, \"edges\": [{\"source_node_id\": \"6fbda1d8\", \"source_node_name\": \"insurance dataset\", \"source_name\": \"data\", \"target_name\": \"raw_data\", \"dst_node_id\": \"b0f7254e\", \"dst_node_name\": \"Prepare Data\"}, {\"source_node_id\": \"b0f7254e\", \"source_node_name\": \"Prepare Data\", \"source_name\": \"prepped_data\", \"target_name\": \"input_509bd521\", \"dst_node_id\": \"de5d27d1\", \"dst_node_name\": \"Train and Register Model\"}], \"child_runs\": [{\"run_id\": \"9990b5aa-0b00-41e0-b1a8-7d8290c80443\", \"name\": \"Prepare Data\", \"status\": \"Running\", \"start_time\": \"\", \"created_time\": \"2023-03-23T07:58:19.560988Z\", \"end_time\": \"\", \"duration\": \"0:01:02\", \"run_number\": 1679558299, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2023-03-23T07:58:19.560988Z\", \"is_reused\": \"\"}, {\"run_id\": \"\", \"name\": \"Train and Register Model\", \"status\": \"NotStarted\", \"start_time\": \"\", \"created_time\": \"\", \"end_time\": \"\", \"duration\": \"\"}]}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.49.0\"}, \"loading\": false}"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "PipelineRunId: 1b4b65a1-19d4-4943-8c5c-f2d5e3f73f2a\nLink to Azure Machine Learning Portal: https://ml.azure.com/runs/1b4b65a1-19d4-4943-8c5c-f2d5e3f73f2a?wsid=/subscriptions/7c248226-48dc-4d36-baa0-0f0883669328/resourcegroups/saurv_01/workspaces/saurav_aml&tid=0563aea9-b886-4b62-9457-a85924a13bd1\nPipelineRun Status: NotStarted\nPipelineRun Status: Running\n\n\nStepRunId: 9990b5aa-0b00-41e0-b1a8-7d8290c80443\nLink to Azure Machine Learning Portal: https://ml.azure.com/runs/9990b5aa-0b00-41e0-b1a8-7d8290c80443?wsid=/subscriptions/7c248226-48dc-4d36-baa0-0f0883669328/resourcegroups/saurv_01/workspaces/saurav_aml&tid=0563aea9-b886-4b62-9457-a85924a13bd1\nStepRun( Prepare Data ) Status: NotStarted\nStepRun( Prepare Data ) Status: Running\n"
        }
      ],
      "execution_count": 22,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1679556372390
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for run in pipeline_run.get_children():\r\n",
        "    print(run.name, ':')\r\n",
        "    metrics = run.get_metrics()\r\n",
        "    for metric_name in metrics:\r\n",
        "        print('\\t',metric_name, \":\", metrics[metric_name])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1679556375191
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Model\r\n",
        "\r\n",
        "for model in Model.list(ws):\r\n",
        "    print(model.name, 'version:', model.version)\r\n",
        "    for tag_name in model.tags:\r\n",
        "        tag = model.tags[tag_name]\r\n",
        "        print ('\\t',tag_name, ':', tag)\r\n",
        "    for prop_name in model.properties:\r\n",
        "        prop = model.properties[prop_name]\r\n",
        "        print ('\\t',prop_name, ':', prop)\r\n",
        "    print('\\n')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1679556376521
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}